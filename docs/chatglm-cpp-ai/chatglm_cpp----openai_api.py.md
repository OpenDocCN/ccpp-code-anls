# `chatglm.cpp\chatglm_cpp\openai_api.py`

```
# å¯¼å…¥å¿…è¦çš„æ¨¡å—
import asyncio
import json
import logging
import time
from typing import Dict, List, Literal, Optional, Union

import chatglm_cpp
import uvicorn
from fastapi import FastAPI, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, computed_field
from pydantic_settings import BaseSettings
from sse_starlette.sse import EventSourceResponse

# é…ç½®æ—¥å¿—è®°å½•å™¨ï¼Œè®¾ç½®æ—¥å¿—çº§åˆ«å’Œæ ¼å¼
logging.basicConfig(level=logging.INFO, format=r"%(asctime)s - %(module)s - %(levelname)s - %(message)s")

# å®šä¹‰é…ç½®ç±»ï¼Œç»§æ‰¿è‡ªBaseSettings
class Settings(BaseSettings):
    model: str = "chatglm3-ggml.bin"
    num_threads: int = 0

# å®šä¹‰å·¥å…·è°ƒç”¨å‡½æ•°æ¨¡å‹
class ToolCallFunction(BaseModel):
    arguments: str
    name: str

# å®šä¹‰å·¥å…·è°ƒç”¨æ¨¡å‹
class ToolCall(BaseModel):
    function: Optional[ToolCallFunction] = None
    type: Literal["function"]

# å®šä¹‰èŠå¤©æ¶ˆæ¯æ¨¡å‹
class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str
    tool_calls: Optional[List[ToolCall]] = None

# å®šä¹‰Deltaæ¶ˆæ¯æ¨¡å‹
class DeltaMessage(BaseModel):
    role: Optional[Literal["system", "user", "assistant"]] = None
    content: Optional[str] = None
    tool_calls: Optional[List[ToolCall]] = None

# å®šä¹‰èŠå¤©å®Œæˆå·¥å…·å‡½æ•°æ¨¡å‹
class ChatCompletionToolFunction(BaseModel):
    description: Optional[str] = None
    name: str
    parameters: Dict

# å®šä¹‰èŠå¤©å®Œæˆå·¥å…·æ¨¡å‹
class ChatCompletionTool(BaseModel):
    type: Literal["function"] = "function"
    function: ChatCompletionToolFunction

# å®šä¹‰èŠå¤©å®Œæˆè¯·æ±‚æ¨¡å‹
class ChatCompletionRequest(BaseModel):
    model: str = "default-model"
    messages: List[ChatMessage]
    temperature: float = Field(default=0.95, ge=0.0, le=2.0)
    top_p: float = Field(default=0.7, ge=0.0, le=1.0)
    stream: bool = False
    max_tokens: int = Field(default=2048, ge=0)
    tools: Optional[List[ChatCompletionTool]] = None

    # å®šä¹‰æ¨¡å‹é…ç½®
    model_config = {
        "json_schema_extra": {"examples": [{"model": "default-model", "messages": [{"role": "user", "content": "ä½ å¥½"}]}]}
    }

# å®šä¹‰èŠå¤©å®Œæˆå“åº”é€‰æ‹©æ¨¡å‹
class ChatCompletionResponseChoice(BaseModel):
    index: int = 0
    message: ChatMessage
    finish_reason: Literal["stop", "length", "function_call"]
# å®šä¹‰ ChatCompletionResponseStreamChoice ç±»ï¼ŒåŒ…å« indexã€delta å’Œ finish_reason å±æ€§
class ChatCompletionResponseStreamChoice(BaseModel):
    index: int = 0
    delta: DeltaMessage
    finish_reason: Optional[Literal["stop", "length"]] = None

# å®šä¹‰ ChatCompletionUsage ç±»ï¼ŒåŒ…å« prompt_tokensã€completion_tokens å’Œ total_tokens å±æ€§
class ChatCompletionUsage(BaseModel):
    prompt_tokens: int
    completion_tokens: int

    # è®¡ç®—å±æ€§ï¼Œè¿”å› prompt_tokens å’Œ completion_tokens ä¹‹å’Œ
    @computed_field
    @property
    def total_tokens(self) -> int:
        return self.prompt_tokens + self.completion_tokens

# å®šä¹‰ ChatCompletionResponse ç±»ï¼ŒåŒ…å« idã€modelã€objectã€createdã€choices å’Œ usage å±æ€§
class ChatCompletionResponse(BaseModel):
    id: str = "chatcmpl"
    model: str = "default-model"
    object: Literal["chat.completion", "chat.completion.chunk"]
    created: int = Field(default_factory=lambda: int(time.time()))
    choices: Union[List[ChatCompletionResponseChoice], List[ChatCompletionResponseStreamChoice]]
    usage: Optional[ChatCompletionUsage] = None

    # å®šä¹‰ model_config å­—å…¸
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "id": "chatcmpl",
                    "model": "default-model",
                    "object": "chat.completion",
                    "created": 1691166146,
                    "choices": [
                        {
                            "index": 0,
                            "message": {"role": "assistant", "content": "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"},
                            "finish_reason": "stop",
                        }
                    ],
                    "usage": {"prompt_tokens": 17, "completion_tokens": 29, "total_tokens": 46},
                }
            ]
        }
    }

# åˆ›å»º Settings å®ä¾‹
settings = Settings()
# åˆ›å»º FastAPI åº”ç”¨å®ä¾‹
app = FastAPI()
# æ·»åŠ  CORS ä¸­é—´ä»¶ï¼Œå…è®¸è·¨åŸŸè¯·æ±‚
app.add_middleware(
    CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)
# åˆ›å»º chatglm_cpp.Pipeline å®ä¾‹
pipeline = chatglm_cpp.Pipeline(settings.model)
# åˆ›å»º asyncio é”å®ä¾‹
lock = asyncio.Lock()

# å®šä¹‰ stream_chat å‡½æ•°ï¼Œç”Ÿæˆ ChatCompletionResponse å¯¹è±¡
def stream_chat(messages, body):
    yield ChatCompletionResponse(
        object="chat.completion.chunk",
        choices=[ChatCompletionResponseStreamChoice(delta=DeltaMessage(role="assistant"))],
    )
    # éå†èŠå¤©ç®¡é“è¿”å›çš„å†…å®¹å—
    for chunk in pipeline.chat(
        messages=messages,  # ä¼ å…¥æ¶ˆæ¯åˆ—è¡¨
        max_length=body.max_tokens,  # æœ€å¤§ç”Ÿæˆé•¿åº¦
        do_sample=body.temperature > 0,  # æ˜¯å¦ä½¿ç”¨é‡‡æ ·
        top_p=body.top_p,  # é¡¶éƒ¨ p å€¼
        temperature=body.temperature,  # æ¸©åº¦å€¼
        num_threads=settings.num_threads,  # çº¿ç¨‹æ•°
        stream=True,  # æ˜¯å¦æµå¼å¤„ç†
    ):
        # è¿”å›èŠå¤©å®Œæˆå“åº”å¯¹è±¡ï¼ŒåŒ…å«å†…å®¹å—
        yield ChatCompletionResponse(
            object="chat.completion.chunk",  # å¯¹è±¡ç±»å‹
            choices=[ChatCompletionResponseStreamChoice(delta=DeltaMessage(content=chunk.content))],  # é€‰æ‹©é¡¹åˆ—è¡¨
        )

    # è¿”å›èŠå¤©å®Œæˆå“åº”å¯¹è±¡ï¼ŒåŒ…å«ç©ºå†…å®¹å—å’Œåœæ­¢åŸå› 
    yield ChatCompletionResponse(
        object="chat.completion.chunk",  # å¯¹è±¡ç±»å‹
        choices=[ChatCompletionResponseStreamChoice(delta=DeltaMessage(), finish_reason="stop")],  # é€‰æ‹©é¡¹åˆ—è¡¨
    )
# å¼‚æ­¥å‡½æ•°ï¼Œç”¨äºå‘å¸ƒèŠå¤©äº‹ä»¶æµ
async def stream_chat_event_publisher(history, body):
    # åˆå§‹åŒ–è¾“å‡ºå­—ç¬¦ä¸²
    output = ""
    try:
        # ä½¿ç”¨é”æ¥ç¡®ä¿çº¿ç¨‹å®‰å…¨
        async with lock:
            # éå†èŠå¤©äº‹ä»¶æµçš„æ¯ä¸ªå—
            for chunk in stream_chat(history, body):
                # æš‚åœå½“å‰åç¨‹ï¼Œå°†æ§åˆ¶æƒäº¤è¿˜ç»™äº‹ä»¶å¾ªç¯ä»¥è¿›è¡Œå–æ¶ˆæ£€æŸ¥
                await asyncio.sleep(0)
                # å°†å½“å‰å—çš„å†…å®¹æ·»åŠ åˆ°è¾“å‡ºä¸­
                output += chunk.choices[0].delta.content or ""
                # è¿”å›å½“å‰å—çš„ JSON æ ¼å¼åŒ–å­—ç¬¦ä¸²
                yield chunk.model_dump_json(exclude_unset=True)
        # è®°å½•æ—¥å¿—ï¼ŒåŒ…å«æœ€åä¸€ä¸ªå†å²è®°å½•å’Œè¾“å‡ºå†…å®¹
        logging.info(f'prompt: "{history[-1]}", stream response: "{output}"')
    except asyncio.CancelledError as e:
        # å¦‚æœå‘ç”Ÿå–æ¶ˆé”™è¯¯ï¼Œè®°å½•æ—¥å¿—ï¼ŒåŒ…å«æœ€åä¸€ä¸ªå†å²è®°å½•å’Œéƒ¨åˆ†è¾“å‡ºå†…å®¹
        logging.info(f'prompt: "{history[-1]}", stream response (partial): "{output}"')
        raise e

# åˆ›å»ºèŠå¤©å®Œæˆè¯·æ±‚çš„è·¯ç”±å¤„ç†å‡½æ•°
@app.post("/v1/chat/completions")
async def create_chat_completion(body: ChatCompletionRequest) -> ChatCompletionResponse:
    # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸º JSON æ ¼å¼çš„å‚æ•°
    def to_json_arguments(arguments):
        def tool_call(**kwargs):
            return kwargs

        try:
            return json.dumps(eval(arguments, dict(tool_call=tool_call)))
        except Exception:
            return arguments

    # å¦‚æœæ¶ˆæ¯ä¸ºç©ºï¼Œåˆ™æŠ›å‡º HTTP å¼‚å¸¸
    if not body.messages:
        raise HTTPException(status.HTTP_400_BAD_REQUEST, "empty messages")

    # å°†è¯·æ±‚ä¸­çš„æ¶ˆæ¯è½¬æ¢ä¸º ChatMessage å¯¹è±¡åˆ—è¡¨
    messages = [chatglm_cpp.ChatMessage(role=msg.role, content=msg.content) for msg in body.messages]
    
    # å¦‚æœå­˜åœ¨å·¥å…·åˆ—è¡¨ï¼Œåˆ™åˆ›å»ºç³»ç»Ÿæ¶ˆæ¯å†…å®¹
    if body.tools:
        system_content = (
            "Answer the following questions as best as you can. You have access to the following tools:\n"
            + json.dumps([tool.model_dump() for tool in body.tools], indent=4)
        )
        # å°†ç³»ç»Ÿæ¶ˆæ¯æ’å…¥åˆ°æ¶ˆæ¯åˆ—è¡¨çš„å¼€å¤´
        messages.insert(0, chatglm_cpp.ChatMessage(role="system", content=system_content))

    # å¦‚æœéœ€è¦æµå¼å¤„ç†ï¼Œåˆ™åˆ›å»ºäº‹ä»¶æµå‘å¸ƒå™¨
    if body.stream:
        generator = stream_chat_event_publisher(messages, body)
        return EventSourceResponse(generator)

    # è®¾ç½®æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
    max_context_length = 512
    # è°ƒç”¨èŠå¤©æ¨¡å‹ï¼Œç”ŸæˆèŠå¤©å®Œæˆçš„è¾“å‡º
    output = pipeline.chat(
        messages=messages,
        max_length=body.max_tokens,
        max_context_length=max_context_length,
        do_sample=body.temperature > 0,
        top_p=body.top_p,
        temperature=body.temperature,
    )
    # è®°å½•æ—¥å¿—ï¼ŒåŒ…å«æœ€åä¸€æ¡æ¶ˆæ¯çš„å†…å®¹å’ŒåŒæ­¥å“åº”çš„å†…å®¹
    logging.info(f'prompt: "{messages[-1].content}", sync response: "{output.content}"')
    # è®¡ç®—æç¤ºæ¶ˆæ¯çš„ token æ•°é‡
    prompt_tokens = len(pipeline.tokenizer.encode_messages(messages, max_context_length))
    # è®¡ç®—å®Œæˆæ¶ˆæ¯çš„ token æ•°é‡
    completion_tokens = len(pipeline.tokenizer.encode(output.content, body.max_tokens))

    # å®ŒæˆåŸå› é»˜è®¤ä¸º "stop"
    finish_reason = "stop"
    tool_calls = None
    # å¦‚æœå­˜åœ¨å·¥å…·è°ƒç”¨
    if output.tool_calls:
        # å°†å·¥å…·è°ƒç”¨è½¬æ¢ä¸º ToolCall å¯¹è±¡åˆ—è¡¨
        tool_calls = [
            ToolCall(
                type=tool_call.type,
                function=ToolCallFunction(
                    name=tool_call.function.name, arguments=to_json_arguments(tool_call.function.arguments)
                ),
            )
            for tool_call in output.tool_calls
        ]
        # å®ŒæˆåŸå› æ›´æ–°ä¸º "function_call"
        finish_reason = "function_call"

    # è¿”å› ChatCompletionResponse å¯¹è±¡
    return ChatCompletionResponse(
        object="chat.completion",
        choices=[
            ChatCompletionResponseChoice(
                message=ChatMessage(role="assistant", content=output.content, tool_calls=tool_calls),
                finish_reason=finish_reason,
            )
        ],
        usage=ChatCompletionUsage(prompt_tokens=prompt_tokens, completion_tokens=completion_tokens),
    )
# å®šä¹‰ä¸€ä¸ªæ¨¡å‹å¡ç‰‡ç±»ï¼Œç»§æ‰¿è‡ªBaseModel
class ModelCard(BaseModel):
    # æ¨¡å‹å¡ç‰‡çš„idå±æ€§ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²
    id: str
    # æ¨¡å‹å¡ç‰‡çš„objectå±æ€§ï¼Œå€¼ä¸º"model"ï¼Œé»˜è®¤ä¸º"model"
    object: Literal["model"] = "model"
    # æ¨¡å‹å¡ç‰‡çš„owned_byå±æ€§ï¼Œå€¼ä¸º"owner"ï¼Œé»˜è®¤ä¸º"owner"
    owned_by: str = "owner"
    # æ¨¡å‹å¡ç‰‡çš„permissionå±æ€§ï¼Œå€¼ä¸ºåˆ—è¡¨ï¼Œä¸ºç©ºåˆ—è¡¨
    permission: List = []

# å®šä¹‰ä¸€ä¸ªæ¨¡å‹åˆ—è¡¨ç±»ï¼Œç»§æ‰¿è‡ªBaseModel
class ModelList(BaseModel):
    # æ¨¡å‹åˆ—è¡¨çš„objectå±æ€§ï¼Œå€¼ä¸º"list"ï¼Œé»˜è®¤ä¸º"list"
    object: Literal["list"] = "list"
    # æ¨¡å‹åˆ—è¡¨çš„dataå±æ€§ï¼Œå€¼ä¸ºæ¨¡å‹å¡ç‰‡åˆ—è¡¨ï¼Œåˆå§‹ä¸ºç©ºåˆ—è¡¨
    data: List[ModelCard] = []

    # æ¨¡å‹åˆ—è¡¨çš„model_configå±æ€§ï¼Œå€¼ä¸ºå­—å…¸ï¼ŒåŒ…å«json_schema_extraå­—æ®µ
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "object": "list",
                    "data": [{"id": "gpt-3.5-turbo", "object": "model", "owned_by": "owner", "permission": []}],
                }
            ]
        }
    }

# å®šä¹‰ä¸€ä¸ªå¼‚æ­¥å‡½æ•°ï¼Œç”¨äºå¤„ç†GETè¯·æ±‚ï¼Œè¿”å›ä¸€ä¸ªæ¨¡å‹åˆ—è¡¨å¯¹è±¡
@app.get("/v1/models")
async def list_models() -> ModelList:
    # è¿”å›ä¸€ä¸ªåŒ…å«ä¸€ä¸ªæ¨¡å‹å¡ç‰‡å¯¹è±¡çš„æ¨¡å‹åˆ—è¡¨å¯¹è±¡
    return ModelList(data=[ModelCard(id="gpt-3.5-turbo")])

# å¦‚æœå½“å‰è„šæœ¬ä½œä¸ºä¸»ç¨‹åºè¿è¡Œï¼Œåˆ™ä½¿ç”¨uvicornè¿è¡Œåº”ç”¨ï¼Œç›‘å¬0.0.0.0:8000åœ°å€ï¼Œä½¿ç”¨1ä¸ªworker
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, workers=1)
```