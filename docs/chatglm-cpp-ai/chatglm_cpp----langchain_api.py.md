# `chatglm.cpp\chatglm_cpp\langchain_api.py`

```
# å¯¼å…¥æ—¥å¿—æ¨¡å—
import logging
# å¯¼å…¥æ—¥æœŸæ—¶é—´æ¨¡å—
from datetime import datetime
# å¯¼å…¥ç±»å‹æç¤ºæ¨¡å—
from typing import List, Tuple

# å¯¼å…¥ chatglm_cpp æ¨¡å—
import chatglm_cpp
# å¯¼å…¥ FastAPI æ¡†æ¶
from fastapi import FastAPI, status
# å¯¼å…¥æ•°æ®éªŒè¯æ¨¡å—
from pydantic import BaseModel, Field
# å¯¼å…¥é…ç½®æ¨¡å—
from pydantic_settings import BaseSettings

# é…ç½®æ—¥å¿—è¾“å‡ºæ ¼å¼
logging.basicConfig(level=logging.INFO, format=r"%(asctime)s - %(module)s - %(levelname)s - %(message)s")

# å®šä¹‰é…ç½®ç±»
class Settings(BaseSettings):
    model: str = "chatglm-ggml.bin"

# å®šä¹‰èŠå¤©è¯·æ±‚æ¨¡å‹
class ChatRequest(BaseModel):
    prompt: str
    history: List[Tuple[str, str]] = []
    max_length: int = Field(default=2048, ge=0)
    top_p: float = Field(default=0.7, ge=0, le=1)
    temperature: float = Field(default=0.95, ge=0, le=2)

    # æ¨¡å‹é…ç½®
    model_config = {"json_schema_extra": {"examples": [{"prompt": "ä½ å¥½"}]}}

# å®šä¹‰èŠå¤©å“åº”æ¨¡å‹
class ChatResponse(BaseModel):
    response: str
    history: List[Tuple[str, str]]
    status: int
    time: str

    # æ¨¡å‹é…ç½®
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "response": "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚",
                    "history": [["ä½ å¥½", "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"]],
                    "status": 200,
                    "time": "2023-08-05 23:01:35",
                }
            ]
        }
    }

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI()
# åŠ è½½é…ç½®
settings = Settings()
# è®°å½•æ—¥å¿—
logging.info(settings)

# åˆ›å»ºèŠå¤©ç®¡é“
pipeline = chatglm_cpp.Pipeline(settings.model)

# å®šä¹‰èŠå¤©æ¥å£
@app.post("/")
async def chat(body: ChatRequest) -> ChatResponse:
    messages = []
    # éå†å†å²æ¶ˆæ¯
    for prompt, response in body.history:
        messages += [
            chatglm_cpp.ChatMessage(role="user", content=prompt),
            chatglm_cpp.ChatMessage(role="assistant", content=response),
        ]
    messages.append(chatglm_cpp.ChatMessage(role="user", content=body.prompt))

    # è¿›è¡ŒèŠå¤©
    output = pipeline.chat(
        messages,
        max_length=body.max_length,
        do_sample=body.temperature > 0,
        top_p=body.top_p,
        temperature=body.temperature,
    )
    # æ›´æ–°å†å²æ¶ˆæ¯
    history = body.history + [(body.prompt, output.content)]
    # åˆ›å»ºä¸€ä¸ªåŒ…å«èŠå¤©å“åº”çš„å¯¹è±¡ï¼ŒåŒ…æ‹¬å“åº”å†…å®¹ã€å†å²è®°å½•ã€çŠ¶æ€ç å’Œæ—¶é—´æˆ³
    answer = ChatResponse(
        response=output.content,
        history=history,
        status=status.HTTP_200_OK,
        time=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    )
    # è®°å½•æ—¥å¿—ï¼ŒåŒ…æ‹¬ç”¨æˆ·æç¤ºå’Œå“åº”å†…å®¹
    logging.info(f'prompt: "{body.prompt}", response: "{output.content}"')
    # è¿”å›èŠå¤©å“åº”å¯¹è±¡
    return answer
```