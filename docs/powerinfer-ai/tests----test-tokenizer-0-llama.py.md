# `PowerInfer\tests\test-tokenizer-0-llama.py`

```
# å¯¼å…¥æ‰€éœ€çš„æ¨¡å—
import os
import sys
import argparse
from sentencepiece import SentencePieceProcessor

# åˆ›å»ºå‚æ•°è§£æå™¨
parser = argparse.ArgumentParser()
# æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
parser.add_argument("dir_tokenizer", help="directory containing 'tokenizer.model' file")
parser.add_argument("--fname-tok",   help="path to a text file to tokenize")
# è§£æå‘½ä»¤è¡Œå‚æ•°
args = parser.parse_args()

# è·å–å‚æ•°å€¼
dir_tokenizer = args.dir_tokenizer

# ä½¿ç”¨æŒ‡å®šçš„ tokenizer.model æ–‡ä»¶åˆ›å»º SentencePieceProcessor å¯¹è±¡
tokenizer = SentencePieceProcessor(dir_tokenizer + '/tokenizer.model')

# å¾…æµ‹è¯•çš„æ–‡æœ¬
tests = [
        "",
        " ",
# ç©ºå­—ç¬¦ä¸²
"  ",
# ç©ºæ ¼
"   ",
# åˆ¶è¡¨ç¬¦
"\t",
# æ¢è¡Œç¬¦
"\n",
# åˆ¶è¡¨ç¬¦å’Œæ¢è¡Œç¬¦ç»„åˆ
"\t\n",
# å­—ç¬¦ä¸² "Hello world"
"Hello world",
# å­—ç¬¦ä¸² " Hello world"
" Hello world",
# å­—ç¬¦ä¸² "Hello World"
"Hello World",
# å­—ç¬¦ä¸² " Hello World"
" Hello World",
# å­—ç¬¦ä¸² " Hello World!"
" Hello World!",
# å­—ç¬¦ä¸² "Hello, world!"
"Hello, world!",
# å­—ç¬¦ä¸² " Hello, world!"
" Hello, world!",
# å­—ç¬¦ä¸² " this is ğŸ¦™.cpp"
" this is ğŸ¦™.cpp",
# å­—ç¬¦ä¸² "w048 7tuijk dsdfhu"
"w048 7tuijk dsdfhu",
# å­—ç¬¦ä¸² "Ğ½ĞµÑ‰Ğ¾ Ğ½Ğ° Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸"
"Ğ½ĞµÑ‰Ğ¾ Ğ½Ğ° Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸",
# å­—ç¬¦ä¸² "á€á¶á“áŸ‹ááŸ‚á–á·áŸáŸáŸá¢á¶á…áá›á…áŸá‰"
"á€á¶á“áŸ‹ááŸ‚á–á·áŸáŸáŸá¢á¶á…áá›á…áŸá‰",
# å­—ç¬¦ä¸² "ğŸš€ (normal) ğŸ˜¶â€ğŸŒ«ï¸ (multiple emojis concatenated) âœ… (only emoji that has its own token)"
"ğŸš€ (normal) ğŸ˜¶â€ğŸŒ«ï¸ (multiple emojis concatenated) âœ… (only emoji that has its own token)",
# å­—ç¬¦ä¸² "Hello"
"Hello",
# å­—ç¬¦ä¸² " Hello"
" Hello",
# å­—ç¬¦ä¸² "  Hello"
"  Hello",
# å®šä¹‰ä¸€ä¸ªåŒ…å«ä¸åŒæ–‡æœ¬çš„åˆ—è¡¨
tests = [
        "   Hello",
        "    Hello",
        "    Hello\n    Hello",
    ]

# éå†æµ‹è¯•æ–‡æœ¬åˆ—è¡¨
for text in tests:
    # æ‰“å°å½“å‰æ–‡æœ¬
    print('text: ', text)
    # æ‰“å°ä½¿ç”¨ bos çš„ç¼–ç ç»“æœ
    print('\nwith bos:')
    print(tokenizer.encode(text, add_bos=True))
    # æ‰“å°ä½¿ç”¨ bos çš„è§£ç ç»“æœ
    print(tokenizer.decode(tokenizer.encode(text, add_bos=True)))
    # æ‰“å°ä¸ä½¿ç”¨ bos çš„ç¼–ç ç»“æœ
    print('\nwithout bos:')
    print(tokenizer.encode(text, add_bos=False))
    # æ‰“å°ä¸ä½¿ç”¨ bos çš„è§£ç ç»“æœ
    print(tokenizer.decode(tokenizer.encode(text, add_bos=False))

# æ‰“å°ç‰¹å®š id å¯¹åº”çš„ token
print("'" + tokenizer.id_to_piece(15043) + "'") # '_Hello'
print("'" + tokenizer.id_to_piece(29871) + "'") # '_'
# æ‰“å°ç‰¹å®š id å¯¹åº”çš„è§£ç ç»“æœ
print("'" + tokenizer.decode([15043]) + "'")        # 'Hello'
print("'" + tokenizer.decode([15043, 15043]) + "'") # 'Hello Hello'
print("'" + tokenizer.decode([29871, 15043]) + "'")               # ' Hello'
# æ‰“å°è§£ç åçš„ç¼–ç ç»“æœï¼ŒåŠ ä¸Šå•å¼•å·
print("'" + tokenizer.decode([29871, 15043, 29871, 15043]) + "'") # ' Hello  Hello'

# æ‰“å° C++ æµ‹è¯•çš„æç¤ºä¿¡æ¯
print("\n\ntests for C++:\n")
# éå†æµ‹è¯•æ–‡æœ¬åˆ—è¡¨
for text in tests:
    # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œä¸æ·»åŠ èµ·å§‹ç¬¦å·
    res = tokenizer.encode(text, add_bos=False)

    # æ›¿æ¢æ–‡æœ¬ä¸­çš„æ¢è¡Œç¬¦å’Œåˆ¶è¡¨ç¬¦ï¼ŒåŠ ä¸ŠåŒå¼•å·
    k = text.replace('\n', '\\n')
    k = k.replace('\t', '\\t')
    k = '"' + k + '"'
    # æ‰“å°ç¼–ç ç»“æœ
    print("{ %-24s, { " % k, end='')
    for x in res:
        print("%7d," % x, end='')
    print(" }, },")

# æ‰“å°ç‰¹å®šæ–‡æœ¬çš„ç¼–ç ç»“æœ
print(tokenizer.encode('hello'))
print(tokenizer.encode('world'))
print(tokenizer.encode(' world'))
print(tokenizer.encode('hello world'))

# è·å–å‚æ•°ä¸­çš„æ–‡ä»¶å
fname_tok = args.fname_tok
# å¦‚æœæ–‡ä»¶åä¸ä¸ºç©º
if fname_tok:
    # æ‰“å°æ­£åœ¨è¿›è¡Œè¯æ±‡æ ‡è®°çš„æ–‡ä»¶å
    print('tokenizing file: ', fname_tok)
    # è®¾ç½®è¾“å‡ºæ–‡ä»¶å
    fname_out = fname_tok + '.tok'
    # æ‰“å¼€æ–‡ä»¶å¹¶è¯»å–å†…å®¹
    with open(fname_tok, 'r', encoding='utf-8') as f:
        # è¯»å–æ–‡ä»¶çš„æ‰€æœ‰è¡Œ
        lines = f.readlines()
        # å°†æ‰€æœ‰è¡Œè¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²
        s = ''.join(lines)
        # ä½¿ç”¨æ ‡è®°å™¨å¯¹å­—ç¬¦ä¸²è¿›è¡Œç¼–ç 
        res = tokenizer.encode(s, add_bos=True)
        # å†™å…¥æ–‡ä»¶
        with open(fname_out, 'w', encoding='utf-8') as f:
            # éå†ç¼–ç ç»“æœå¹¶å†™å…¥æ–‡ä»¶
            for x in res:
                f.write(str(x) + ' \'' + tokenizer.decode(x) + '\'\n')
        # æ‰“å°ç¼–ç ç»“æœçš„é•¿åº¦
        print('len(res): ', len(res))
        # æ‰“å°æ–‡ä»¶è¡Œæ•°
        print('len(lines): ', len(lines))
    # æ‰“å°ç»“æœå†™å…¥çš„æ–‡ä»¶å
    print('results written to: ', fname_out)
```