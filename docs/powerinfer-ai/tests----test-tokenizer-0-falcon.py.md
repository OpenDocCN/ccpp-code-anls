# `PowerInfer\tests\test-tokenizer-0-falcon.py`

```
# å¯¼å…¥æ‰€éœ€çš„åº“
import os
import sys
import argparse

# ä»transformersåº“ä¸­å¯¼å…¥AutoTokenizerç±»
from transformers import AutoTokenizer

# åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
parser = argparse.ArgumentParser()
# æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šåŒ…å«'tokenizer.model'æ–‡ä»¶çš„ç›®å½•
parser.add_argument("dir_tokenizer", help="directory containing 'tokenizer.model' file")
# æ·»åŠ å¯é€‰å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šè¦è¿›è¡Œæ ‡è®°åŒ–çš„æ–‡æœ¬æ–‡ä»¶çš„è·¯å¾„
parser.add_argument("--fname-tok",   help="path to a text file to tokenize")
# è§£æå‘½ä»¤è¡Œå‚æ•°
args = parser.parse_args()

# è·å–å‘½ä»¤è¡Œå‚æ•°ä¸­æŒ‡å®šçš„ç›®å½•è·¯å¾„
dir_tokenizer = args.dir_tokenizer

# ä½¿ç”¨AutoTokenizerç±»ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(dir_tokenizer)

# å‡†å¤‡è¿›è¡Œæ ‡è®°åŒ–çš„æµ‹è¯•æ–‡æœ¬
tests = [
        "",
        " ",
# ç©ºå­—ç¬¦ä¸²
"  ",
# ç©ºå­—ç¬¦ä¸²
"   ",
# åˆ¶è¡¨ç¬¦
"\t",
# æ¢è¡Œç¬¦
"\n",
# åˆ¶è¡¨ç¬¦å’Œæ¢è¡Œç¬¦ç»„åˆ
"\t\n",
# å­—ç¬¦ä¸² "Hello world"
"Hello world",
# å­—ç¬¦ä¸² " Hello world"
" Hello world",
# å­—ç¬¦ä¸² "Hello World"
"Hello World",
# å­—ç¬¦ä¸² " Hello World"
" Hello World",
# å­—ç¬¦ä¸² " Hello World!"
" Hello World!",
# å­—ç¬¦ä¸² "Hello, world!"
"Hello, world!",
# å­—ç¬¦ä¸² " Hello, world!"
" Hello, world!",
# å­—ç¬¦ä¸² " this is ğŸ¦™.cpp"
" this is ğŸ¦™.cpp",
# å­—ç¬¦ä¸² "w048 7tuijk dsdfhu"
"w048 7tuijk dsdfhu",
# å­—ç¬¦ä¸² "Ğ½ĞµÑ‰Ğ¾ Ğ½Ğ° Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸"
"Ğ½ĞµÑ‰Ğ¾ Ğ½Ğ° Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸",
# å­—ç¬¦ä¸² "á€á¶á“áŸ‹ááŸ‚á–á·áŸáŸáŸá¢á¶á…áá›á…áŸá‰"
"á€á¶á“áŸ‹ááŸ‚á–á·áŸáŸáŸá¢á¶á…áá›á…áŸá‰",
# å­—ç¬¦ä¸² "ğŸš€ (normal) ğŸ˜¶â€ğŸŒ«ï¸ (multiple emojis concatenated) âœ… (only emoji that has its own token)"
"ğŸš€ (normal) ğŸ˜¶â€ğŸŒ«ï¸ (multiple emojis concatenated) âœ… (only emoji that has its own token)",
# å­—ç¬¦ä¸² "Hello"
"Hello",
# å­—ç¬¦ä¸² " Hello"
" Hello",
# å­—ç¬¦ä¸² "  Hello"
"  Hello",
# å®šä¹‰ä¸€ä¸ªåŒ…å«å¤šä¸ªå­—ç¬¦ä¸²çš„åˆ—è¡¨
tests = [
        "   Hello",
        "    Hello",
        "    Hello\n    Hello",
        "\n =",
        "' era",
    ]

# éå†åˆ—è¡¨ä¸­çš„æ¯ä¸ªå­—ç¬¦ä¸²
for text in tests:
    # æ‰“å°å½“å‰å­—ç¬¦ä¸²
    print('text: ', text)
    # å¯¹å½“å‰å­—ç¬¦ä¸²è¿›è¡Œç¼–ç 
    print(tokenizer.encode(text))
    # å¯¹ç¼–ç åçš„ç»“æœè¿›è¡Œè§£ç 
    print(tokenizer.decode(tokenizer.encode(text)))

# æ‰“å° C++ æµ‹è¯•çš„æ ‡è®°
print("\n\ntests for C++:\n")
# éå†åˆ—è¡¨ä¸­çš„æ¯ä¸ªå­—ç¬¦ä¸²
for text in tests:
    # å¯¹å½“å‰å­—ç¬¦ä¸²è¿›è¡Œç¼–ç 
    res = tokenizer.encode(text)

    # æ›¿æ¢æ¢è¡Œç¬¦å’Œåˆ¶è¡¨ç¬¦
    k = text.replace('\n', '\\n')
    k = k.replace('\t', '\\t')
    # åœ¨å­—ç¬¦ä¸²å‰åæ·»åŠ å¼•å·
    k = '"' + k + '"'
    # æ‰“å°æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
    print("{ %-24s, { " % k, end='')
# éå† res åˆ—è¡¨ä¸­çš„å…ƒç´ ï¼Œå¹¶æŒ‰æŒ‡å®šæ ¼å¼æ‰“å°å‡ºæ¥
for x in res:
    print("%7d," % x, end='')
# æ‰“å°ç»“æŸç¬¦
print(" }, },")
# æ‰“å°ä½¿ç”¨ tokenizer å¯¹è±¡ç¼–ç çš„å­—ç¬¦ä¸²
print(tokenizer.encode('hello'))
print(tokenizer.encode('world'))
print(tokenizer.encode(' world'))
print(tokenizer.encode('hello world'))
# æ£€æŸ¥æ˜¯å¦å­˜åœ¨ fname_tok æ–‡ä»¶å
fname_tok = args.fname_tok
# å¦‚æœå­˜åœ¨ï¼Œåˆ™è¿›è¡Œæ–‡ä»¶åˆ†è¯å¤„ç†
if fname_tok:
    print('tokenizing file: ', fname_tok)
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    fname_out = fname_tok + '.tok'
    # æ‰“å¼€æ–‡ä»¶ï¼Œè¯»å–å†…å®¹
    with open(fname_tok, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        s = ''.join(lines)
        # ä½¿ç”¨ tokenizer å¯¹è±¡å¯¹å†…å®¹è¿›è¡Œç¼–ç 
        res = tokenizer.encode(s)
        # å†™å…¥æ–‡ä»¶
        with open(fname_out, 'w', encoding='utf-8') as f:
            # éå†ç¼–ç ç»“æœï¼Œå¹¶å†™å…¥æ–‡ä»¶
            for x in res:
# å°†å˜é‡ x è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶å†™å…¥æ–‡ä»¶ï¼ŒåŒæ—¶å†™å…¥ token è§£ç åçš„å†…å®¹
f.write(str(x) + ' \'' + tokenizer.decode(x) + '\'\n')
# æ‰“å° res åˆ—è¡¨çš„é•¿åº¦
print('len(res): ', len(res))
# æ‰“å° lines åˆ—è¡¨çš„é•¿åº¦
print('len(lines): ', len(lines))
# æ‰“å°è¾“å‡ºç»“æœæ–‡ä»¶çš„è·¯å¾„
print('results written to: ', fname_out)
```