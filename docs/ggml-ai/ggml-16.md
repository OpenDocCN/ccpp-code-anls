# GGMLæºç è§£æ 16

# `examples/starcoder/quantize.cpp`

è¿™æ®µä»£ç æ˜¯ä¸€ä¸ªC++ç¨‹åºï¼Œä¸»è¦ä½œç”¨æ˜¯è¯»å–ä¸€ä¸ªå®šä¹‰å¥½çš„ç»“æ„ä½“æ•°ç»„(ggml::Vector)ï¼Œå¹¶å°†å…¶ä¸­çš„å…ƒç´ æ‰“å°å‡ºæ¥ã€‚ç»“æ„ä½“æ•°ç»„çš„åå­—ä¸º"common.h"ï¼Œè¿™é‡Œæˆ‘ä»¬å‡è®¾è¿™ä¸ªç»“æ„ä½“æœ‰å¤šä¸ªæˆå‘˜å˜é‡ã€‚

ç¨‹åºé¦–å…ˆå¼•å…¥äº†ggml::ggml.hå’Œcommon.hä¸¤ä¸ªå¤´æ–‡ä»¶ï¼Œggml::ggml.hå¯èƒ½æ˜¯ä¸€ä¸ªç±»ä¼¼äºggmlåº“çš„å¤´æ–‡ä»¶ï¼Œcommon.håˆ™æ˜¯è¿™ä¸ªç¨‹åºè‡ªå·±çš„å¤´æ–‡ä»¶ï¼Œå¾ˆå¯èƒ½æ˜¯å®šä¹‰äº†mainå‡½æ•°ã€‚

æ¥ç€ï¼Œç¨‹åºé€šè¿‡common.hå¤´æ–‡ä»¶ä¸­çš„å‡½æ•°gcm_load_path(std::string& file_path)è¯»å–äº†ä¸€ä¸ªå®šä¹‰å¥½çš„ggml::Vectorå¯¹è±¡ï¼Œå¹¶å°†å…¶å­˜å‚¨åˆ°äº†std::vector<ggml::Vector>å®¹å™¨ä¸­ã€‚ç„¶åï¼Œç¨‹åºé€šè¿‡å®¹å™¨ä¸­çš„å‡½æ•°print_vectors(const std::vector<ggml::Vector>& vectors)æ‰“å°å‡ºäº†è¿™ä¸ªggml::Vectorå®¹å™¨ä¸­çš„æ‰€æœ‰å…ƒç´ ã€‚

ä¸è¿‡ï¼Œç¨‹åºåœ¨è¯»å–æ–‡ä»¶æ—¶éœ€è¦æä¾›ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº†ggml::ggml.hä¸­çš„å‡½æ•°ggml_open_file(const std::string& file_path)æ¥è¯»å–æ–‡ä»¶ã€‚regexæ˜¯C++11ä¸­çš„ä¸€ä¸ªæ ‡å‡†åº“ï¼Œå…¶ä¸­åŒ…å«äº†ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼ï¼Œå¯ä»¥ç”¨äºå­—ç¬¦ä¸²çš„åŒ¹é…ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨regexæ¥åˆ¤æ–­ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦ä¸º"static_exports"ã€‚


```cpp
#include "ggml/ggml.h"

#include "common.h"
#include "common-ggml.h"

#include <cassert>
#include <cmath>
#include <cstdio>
#include <cstring>
#include <fstream>
#include <map>
#include <string>
#include <vector>
#include <regex>

```

This is a function that reads a model file andits attributes, such as the number of vocabulary elements and the size of the model. It reads the model file and its attributes from the input file, such as the name of the file and its size, and then reads the model file.

This function reads the text data of the model file and stores it in the input buffer. It then checks if the file size is equal to the size of the model file specified by the user. If the file size is not equal to the specified size, the function will print an error message and return false.

The function also reads the id of the word and its corresponding index in the vocabulary. After that, it prints the remaining words in the file if the user has not specified any specific options for quantization.

Finally, the function uses a common quantization method to quantize the model file and returns true if the quantization was successful. If the quantization fails, the function will print an error message and return false.


```cpp
// default hparams (GPT-2 117M)
struct starcoder_hparams {
    int32_t n_vocab = 49280;
    int32_t n_ctx   = 2048;
    int32_t n_embd  = 2048;
    int32_t n_head  = 16;
    int32_t n_layer = 24;
    int32_t ftype   = 1;
};

// quantize a model
bool starcoder_model_quantize(const std::string & fname_inp, const std::string & fname_out, ggml_ftype ftype) {
    gpt_vocab vocab;

    printf("%s: loading model from '%s'\n", __func__, fname_inp.c_str());

    auto finp = std::ifstream(fname_inp, std::ios::binary);
    if (!finp) {
        fprintf(stderr, "%s: failed to open '%s' for reading\n", __func__, fname_inp.c_str());
        return false;
    }

    auto fout = std::ofstream(fname_out, std::ios::binary);
    if (!fout) {
        fprintf(stderr, "%s: failed to open '%s' for writing\n", __func__, fname_out.c_str());
        return false;
    }

    // verify magic
    {
        uint32_t magic;
        finp.read((char *) &magic, sizeof(magic));
        if (magic != GGML_FILE_MAGIC) {
            fprintf(stderr, "%s: invalid model file '%s' (bad magic)\n", __func__, fname_inp.c_str());
            return false;
        }

        fout.write((char *) &magic, sizeof(magic));
    }

    starcoder_hparams hparams;

    // load hparams
    {
        finp.read((char *) &hparams.n_vocab, sizeof(hparams.n_vocab));
        finp.read((char *) &hparams.n_ctx,   sizeof(hparams.n_ctx));
        finp.read((char *) &hparams.n_embd,  sizeof(hparams.n_embd));
        finp.read((char *) &hparams.n_head,  sizeof(hparams.n_head));
        finp.read((char *) &hparams.n_layer, sizeof(hparams.n_layer));
        finp.read((char *) &hparams.ftype,   sizeof(hparams.ftype));

        const int32_t qntvr_src =    hparams.ftype / GGML_QNT_VERSION_FACTOR;
        const int32_t ftype_dst = GGML_QNT_VERSION * GGML_QNT_VERSION_FACTOR + ftype;

        printf("%s: n_vocab     = %d\n", __func__, hparams.n_vocab);
        printf("%s: n_ctx       = %d\n", __func__, hparams.n_ctx);
        printf("%s: n_embd      = %d\n", __func__, hparams.n_embd);
        printf("%s: n_head      = %d\n", __func__, hparams.n_head);
        printf("%s: n_layer     = %d\n", __func__, hparams.n_layer);
        printf("%s: ftype (src) = %d\n", __func__, hparams.ftype);
        printf("%s: qntvr (src) = %d\n", __func__, qntvr_src);
        printf("%s: ftype (dst) = %d\n", __func__, ftype_dst);
        printf("%s: qntvr (dst) = %d\n", __func__, GGML_QNT_VERSION);

        fout.write((char *) &hparams.n_vocab, sizeof(hparams.n_vocab));
        fout.write((char *) &hparams.n_ctx,   sizeof(hparams.n_ctx));
        fout.write((char *) &hparams.n_embd,  sizeof(hparams.n_embd));
        fout.write((char *) &hparams.n_head,  sizeof(hparams.n_head));
        fout.write((char *) &hparams.n_layer, sizeof(hparams.n_layer));
        fout.write((char *) &ftype_dst,       sizeof(ftype_dst));
    }

    // load vocab
    {
        int32_t n_vocab = 0;
        finp.read ((char *) &n_vocab, sizeof(n_vocab));
        fout.write((char *) &n_vocab, sizeof(n_vocab));

        if (n_vocab != hparams.n_vocab) {
            fprintf(stderr, "%s: invalid model file '%s' (bad vocab size %d != %d)\n",
                    __func__, fname_inp.c_str(), n_vocab, hparams.n_vocab);
            return false;
        }

        std::string word;
        for (int i = 0; i < n_vocab; i++) {
            uint32_t len;
            finp.read ((char *) &len, sizeof(len));
            fout.write((char *) &len, sizeof(len));

            word.resize(len);
            finp.read ((char *) word.data(), len);
            fout.write((char *) word.data(), len);

            vocab.token_to_id[word] = i;
            vocab.id_to_token[i] = word;
        }
    }

    // regexes of tensor names to be quantized
    const std::vector<std::string> to_quant = {
        "model/wte",
        "model/lm_head",
        "model/h.*/attn/c_attn/w",
        "model/h.*/attn/c_proj/w",
        "model/h.*/mlp/c_fc/w",
        "model/h.*/mlp/c_proj/w",
    };

    if (!ggml_common_quantize_0(finp, fout, ftype, to_quant, {})) {
        fprintf(stderr, "%s: failed to quantize model '%s'\n", __func__, fname_inp.c_str());
        return false;
    }

    finp.close();
    fout.close();

    return true;
}

```

This is a C++ program that appears to quantize a machine learning model represented in the F16 format. The program takes two command-line arguments: a file name for the input model and a file name for the output quantized model.

The program first checks the input file and reports any errors. It then initializes a feedback filter for the quantized model, which is used to filter the input and keep track of the quantized data.

The program then quantizes the input model by calling the `starcoder_model_quantize` function, which takes the name of the input file, the name of the output file, and the F16 type of the model. The function returns the quantized model in the output file.

Finally, the program reports the timing information for the quantization process and returns 0 if the quantization was successful, or 1 if it failed.


```cpp
// usage:
//  ./gpt-2-quantize models/gpt-2-117M/ggml-model.bin models/gpt-2-117M/ggml-model-quant.bin type
//
int main(int argc, char ** argv) {
    if (argc != 4) {
        fprintf(stderr, "usage: %s model-f32.bin model-quant.bin type\n", argv[0]);
        ggml_print_ftypes(stderr);
        return 1;
    }

    // needed to initialize f16 tables
    {
        struct ggml_init_params params = { 0, NULL, false };
        struct ggml_context * ctx = ggml_init(params);
        ggml_free(ctx);
    }

    const std::string fname_inp = argv[1];
    const std::string fname_out = argv[2];

    const ggml_ftype ftype = ggml_parse_ftype(argv[3]);

    const int64_t t_main_start_us = ggml_time_us();

    int64_t t_quantize_us = 0;

    // load the model
    {
        const int64_t t_start_us = ggml_time_us();

        if (!starcoder_model_quantize(fname_inp, fname_out, ggml_ftype(ftype))) {
            fprintf(stderr, "%s: failed to quantize model from '%s'\n", __func__, fname_inp.c_str());
            return 1;
        }

        t_quantize_us = ggml_time_us() - t_start_us;
    }

    // report timing
    {
        const int64_t t_main_end_us = ggml_time_us();

        printf("\n");
        printf("%s: quantize time = %8.2f ms\n", __func__, t_quantize_us/1000.0f);
        printf("%s:    total time = %8.2f ms\n", __func__, (t_main_end_us - t_main_start_us)/1000.0f);
    }

    return 0;
}

```

# ğŸ’« StarCoder

This is a C++ example running ğŸ’« StarCoder inference using the [ggml](https://github.com/ggerganov/ggml) library.

The program runs on the CPU - no video card is required.

The example supports the following ğŸ’« StarCoder models:

- `bigcode/starcoder`
- `bigcode/gpt_bigcode-santacoder` aka the smol StarCoder

Sample performance on MacBook M1 Pro:

TODO


Sample output:

```cpp
$ ./bin/starcoder -h
usage: ./bin/starcoder [options]

options:
  -h, --help            show this help message and exit
  -s SEED, --seed SEED  RNG seed (default: -1)
  -t N, --threads N     number of threads to use during computation (default: 8)
  -p PROMPT, --prompt PROMPT
                        prompt to start generation with (default: random)
  -n N, --n_predict N   number of tokens to predict (default: 200)
  --top_k N             top-k sampling (default: 40)
  --top_p N             top-p sampling (default: 0.9)
  --temp N              temperature (default: 1.0)
  -b N, --batch_size N  batch size for prompt processing (default: 8)
  -m FNAME, --model FNAME
                        model path (default: models/starcoder-117M/ggml-model.bin)

$ ./bin/starcoder -m ../models/bigcode/gpt_bigcode-santacoder-ggml-q4_1.bin -p "def fibonnaci(" -t 4 --top_k 0 --top_p 0.95 --temp 0.2      
main: seed = 1683881276
starcoder_model_load: loading model from '../models/bigcode/gpt_bigcode-santacoder-ggml-q4_1.bin'
starcoder_model_load: n_vocab = 49280
starcoder_model_load: n_ctx   = 2048
starcoder_model_load: n_embd  = 2048
starcoder_model_load: n_head  = 16
starcoder_model_load: n_layer = 24
starcoder_model_load: ftype   = 3
starcoder_model_load: ggml ctx size = 1794.90 MB
starcoder_model_load: memory size =   768.00 MB, n_mem = 49152
starcoder_model_load: model size  =  1026.83 MB
main: prompt: 'def fibonnaci('
main: number of tokens in prompt = 7, first 8 tokens: 563 24240 78 2658 64 2819 7 

def fibonnaci(n):
    if n == 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fibonacci(n-1) + fibonacci(n-2)

print(fibo(10))

main: mem per token =  9597928 bytes
main:     load time =   480.43 ms
main:   sample time =    26.21 ms
main:  predict time =  3987.95 ms / 19.36 ms per token
main:    total time =  4580.56 ms
```

## Quick start
```cppbash
git clone https://github.com/ggerganov/ggml
cd ggml

# Install Python dependencies
python3 -m pip install -r requirements.txt

# Convert HF model to ggml
python examples/starcoder/convert-hf-to-ggml.py bigcode/gpt_bigcode-santacoder

# Build ggml + examples
mkdir build && cd build
cmake .. && make -j4 starcoder starcoder-quantize

# quantize the model
./bin/starcoder-quantize ../models/bigcode/gpt_bigcode-santacoder-ggml.bin ../models/bigcode/gpt_bigcode-santacoder-ggml-q4_1.bin 3

# run inference
./bin/starcoder -m ../models/bigcode/gpt_bigcode-santacoder-ggml-q4_1.bin -p "def fibonnaci(" --top_k 0 --top_p 0.95 --temp 0.2
```


## Downloading and converting the original models (ğŸ’« StarCoder)

You can download the original model and convert it to `ggml` format using the script `convert-hf-to-ggml.py`:

```cpp
# Convert HF model to ggml
python examples/starcoder/convert-hf-to-ggml.py bigcode/gpt_bigcode-santacoder
```

This conversion requires that you have python and Transformers installed on your computer.

## Quantizing the models

You can also try to quantize the `ggml` models via 4-bit integer quantization.

```cpp
# quantize the model
./bin/starcoder-quantize ../models/bigcode/gpt_bigcode-santacoder-ggml.bin ../models/bigcode/gpt_bigcode-santacoder-ggml-q4_1.bin 3
```

| Model | Original size | Quantized size | Quantization type |
| --- | --- | --- | --- |
| `bigcode/gpt_bigcode-santacoder` | 5396.45 MB | 1026.83 MB | 4-bit integer (q4_1) |
| `bigcode/starcoder` | 71628.23 MB | 13596.23 MB | 4-bit integer (q4_1) |


# `examples/starcoder/starcoder-mmap.cpp`

è¿™æ®µä»£ç æ˜¯ä¸€ä¸ªC++ç¨‹åºï¼Œå®ƒåŒ…æ‹¬äº†ä¸¤ä¸ªå¤´æ–‡ä»¶ï¼šggml.hå’Œcommon.hã€‚ggml.hå’Œcommon.héƒ½åŒ…å«äº†å¤´æ–‡ä»¶ggml.hå’Œcommon.hï¼Œè¿™ä¸¤ä¸ªå¤´æ–‡ä»¶æ²¡æœ‰å®šä¹‰å‡½æ•°ï¼Œä½†å®ƒä»¬çš„å£°æ˜åœ¨å‡½æ•°çš„å‰é¢ã€‚

è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯ï¼šå®šä¹‰äº†ä¸€ä¸ªåä¸ºâ€œcommonâ€çš„ç±»ï¼Œå®ƒåŒ…æ‹¬äº†ä¸€ä¸ªåä¸ºâ€œggmlâ€çš„å‡½æ•°å’Œä¸€ä¸ªåä¸ºâ€œcommon_ggmlâ€çš„å‡½æ•°ã€‚ggmlå‡½æ•°çš„ä½œç”¨æ˜¯â€œggmlâ€ï¼Œcommon_ggmlå‡½æ•°çš„ä½œç”¨æ˜¯â€œcommon_ggmlâ€ã€‚


```cpp
#include "ggml/ggml.h"

#include "common.h"
#include "common-ggml.h"

#include <cassert>
#include <cmath>
#include <cstdio>
#include <cstring>
#include <fstream>
#include <map>
#include <string>
#include <vector>

#if !defined(_WIN32)
```

è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º "mmap" çš„å‡½æ•°ï¼Œå®ƒçš„ä½œç”¨æ˜¯æ¨¡æ‹Ÿæ“ä½œç³»ç»Ÿä¸­çš„ mmap å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°å¯ä»¥ç”¨æ¥åŠ¨æ€åœ°æ˜ å°„ä¸€ä¸ªæ–‡ä»¶åˆ°å†…å­˜ä¸­ï¼Œç„¶åé€šè¿‡æŒ‡é’ˆ p è®¿é—®æ˜ å°„çš„å†…å­˜åŒºåŸŸã€‚

å…·ä½“æ¥è¯´ï¼Œè¿™æ®µä»£ç åŒ…å«ä»¥ä¸‹å‡ è¡Œï¼š

1. `#include <sys/types.h>`ï¼šå¼•å…¥äº† `sys/types.h` å¤´æ–‡ä»¶ï¼Œå¯èƒ½æ˜¯ç”¨äºå®šä¹‰ `typedef` æˆ–è€… `const` ç±»å‹çš„å®å®šä¹‰ã€‚
2. `#include <sys/mman.h>`ï¼šå¼•å…¥äº† `sys/mman.h` å¤´æ–‡ä»¶ï¼Œå¯èƒ½æ˜¯ç”¨äºæ¨¡æ‹Ÿæ“ä½œç³»ç»Ÿä¸­çš„ mmap å‡½æ•°ã€‚
3. `#include <unistd.h>`ï¼šå¼•å…¥äº† `unistd.h` å¤´æ–‡ä»¶ï¼Œå¯èƒ½æ˜¯ç”¨äºåŒ…å«ä¸€ä¸ªé€šç”¨çš„æ–‡ä»¶æ“ä½œå‡½æ•°ã€‚
4. `#include <fcntl.h>`ï¼šå¼•å…¥äº† `fcntl.h` å¤´æ–‡ä»¶ï¼Œå¯èƒ½æ˜¯ç”¨äºåŒ…å«ä¸€ä¸ªé€šç”¨çš„æ–‡ä»¶æ“ä½œå‡½æ•°ã€‚
5. `#ifdef GGML_USE_CUBLAS`ï¼šå¦‚æœè¿™ä¸ªæ¡ä»¶ä¸ºçœŸï¼Œé‚£ä¹ˆä¼šåŒ…å«ä¸€ä¸ªåä¸º "ggml-cuda.h" çš„å¤´æ–‡ä»¶ï¼Œå¾ˆå¯èƒ½æ˜¯ç”¨äºä¸ CUDA åº“è¿›è¡Œäº¤äº’çš„ã€‚
6. `#endif`ï¼šç”¨äºå°†æ¡ä»¶ä»£ç å—å’Œä¸éœ€è¦çš„ä»£ç åˆ†ç¦»å¼€æ¥ï¼Œèµ·åˆ°å¢å¼ºä»£ç å¯è¯»æ€§çš„ä½œç”¨ã€‚

å¦å¤–ï¼Œç”±äº `#ifdef` å’Œ `#define` ä¹‹é—´çš„ä»£ç å—æ²¡æœ‰åˆ†å·ï¼Œå› æ­¤å®ƒä»¬åº”è¯¥è¢«è§†ä¸ºä¸€ä¸ªæ•´ä½“ï¼Œä½œä¸ºä¸€ä¸ªç®€å•çš„ä»£ç ç‰‡æ®µã€‚


```cpp
// mmap
#include <sys/types.h>
#include <sys/mman.h>
#include <unistd.h>
#include <fcntl.h>
#else
#define NOMINMAX
#include <Windows.h>
#endif

#ifdef GGML_USE_CUBLAS
#include "ggml-cuda.h"
#endif

#ifdef GGML_USE_CLBLAST
```

è¿™æ®µä»£ç åŒ…æ‹¬ä¸¤ä¸ªéƒ¨åˆ†ï¼š

1. å¤´æ–‡ä»¶åŒ…å«ï¼šggml-opencl.hï¼Œè¿™æ˜¯ä¸€ä¸ªC++å¤´æ–‡ä»¶ï¼Œå®ƒå¼•å…¥äº†ggml-openclåº“ï¼Œç”¨äºåœ¨C++ä¸­ä½¿ç”¨GPT-2æ¨¡å‹çš„OpenCLç‰ˆæœ¬ã€‚

2. å®šä¹‰ç»“æ„ä½“ï¼šstarcoder_hparamsï¼Œè¯¥ç»“æ„ä½“å®šä¹‰äº†å‚æ•°åŒ–çš„å“ˆå¸Œå‚æ•°ï¼ŒåŒ…æ‹¬è¯æ±‡è¡¨å¤§å°ï¼ˆn_vocabï¼‰ã€ä¸Šä¸‹æ–‡å¤§å°ï¼ˆn_ctxï¼‰ã€åµŒå…¥ç»´åº¦å¤§å°ï¼ˆn_embdï¼‰ã€å¤´æ•°ï¼ˆn_headï¼‰ã€å±‚æ•°ï¼ˆn_layerï¼‰å’Œæ–‡ä»¶ç±»å‹ï¼ˆftypeï¼‰ï¼Œä»¥åŠæµ®ç‚¹æ•°ç±»å‹ï¼ˆfloatï¼‰çš„é‚»åŸŸï¼ˆepsï¼‰å‚æ•°ã€‚è¿™äº›å‚æ•°ç”¨äºé…ç½®æ¨¡å‹æ¶æ„å’Œä¼˜åŒ–è®¾ç½®ã€‚


```cpp
#include "ggml-opencl.h"
#endif

// default hparams (GPT-2 117M)
// https://huggingface.co/bigcode/gpt_bigcode-santacoder/blob/main/config.json
struct starcoder_hparams {
    int32_t n_vocab = 49280;
    int32_t n_ctx   = 2048;
    int32_t n_embd  = 2048;
    int32_t n_head  = 16;
    int32_t n_layer = 24;
    int32_t ftype   = 1;
    float   eps     = 1e-5f;
};

```

è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º `starcoder_layer` çš„ç»“æ„ä½“ï¼Œè¡¨ç¤ºåœ¨ä¸€ä¸ªç¥ç»ç½‘ç»œçš„å±‚ä¸­ã€‚è¿™ä¸ªç»“æ„ä½“åŒ…å«äº†ä¸€äº›ç”¨äºå±‚è¾“å‡ºçš„å‚æ•°ã€‚

é¦–å…ˆï¼Œå®ƒå®šä¹‰äº†ä¸€ä¸ªæ ‡é‡ç±»å‹ä¸º `struct ggml_tensor *` çš„ `ln_1_g` å’Œ `ln_1_b` æˆå‘˜å˜é‡ï¼Œå®ƒä»¬å°†ç”¨äºå¯¹è¾“å…¥æ•°æ®è¿›è¡Œå½’ä¸€åŒ–æ“ä½œã€‚

æ¥ä¸‹æ¥ï¼Œå®ƒå®šä¹‰äº†ä¸€ä¸ªæ ‡é‡ç±»å‹ä¸º `struct ggml_tensor *` çš„ `ln_2_g` å’Œ `ln_2_b` æˆå‘˜å˜é‡ï¼Œå®ƒä»¬å°†ç”¨äºè®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚

ç„¶åï¼Œå®ƒå®šä¹‰äº†ä¸€ä¸ªæ ‡é‡ç±»å‹ä¸º `struct ggml_tensor *` çš„ `c_attn_attn_w` å’Œ `c_attn_attn_b` æˆå‘˜å˜é‡ï¼Œå®ƒä»¬å°†ç”¨äºè®¡ç®—æ³¨æ„åŠ›ã€‚

æ¥ç€ï¼Œå®ƒå®šä¹‰äº†ä¸€ä¸ªæ ‡é‡ç±»å‹ä¸º `struct ggml_tensor *` çš„ `c_attn_proj_w` å’Œ `c_attn_proj_b` æˆå‘˜å˜é‡ï¼Œå®ƒä»¬å°†ç”¨äºè®¡ç®—å·ç§¯å±‚ä¸­çš„æŠ•å½±æƒé‡ã€‚

æœ€åï¼Œå®ƒå®šä¹‰äº†ä¸€ä¸ªæ ‡é‡ç±»å‹ä¸º `struct ggml_tensor *` çš„ `c_mlp_fc_w` å’Œ `c_mlp_fc_b` æˆå‘˜å˜é‡ï¼Œå®ƒä»¬å°†ç”¨äºè®¡ç®—å…¨è¿æ¥å±‚çš„è¾“å‡ºå‚æ•°ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¿™ä¸ªç»“æ„ä½“è¡¨ç¤ºä¸€ä¸ªç¥ç»ç½‘ç»œçš„å±‚ï¼Œå…¶ä¸­çš„å‚æ•°ç”¨äºå¯¹è¾“å…¥æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ã€è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€è®¡ç®—å·ç§¯å±‚ä¸­çš„æŠ•å½±æƒé‡å’Œå…¨è¿æ¥å±‚çš„è¾“å‡ºå‚æ•°ã€‚


```cpp
struct starcoder_layer {
    // normalization
    struct ggml_tensor * ln_1_g;
    struct ggml_tensor * ln_1_b;

    struct ggml_tensor * ln_2_g;
    struct ggml_tensor * ln_2_b;

    // attention
    struct ggml_tensor * c_attn_attn_w;
    struct ggml_tensor * c_attn_attn_b;

    struct ggml_tensor * c_attn_proj_w;
    struct ggml_tensor * c_attn_proj_b;

    // mlp
    struct ggml_tensor * c_mlp_fc_w;
    struct ggml_tensor * c_mlp_fc_b;

    struct ggml_tensor * c_mlp_proj_w;
    struct ggml_tensor * c_mlp_proj_b;
};

```

è¿™æ˜¯ä¸€ä¸ªç»“æ„ä½“å˜é‡ `llama_buffer`ï¼Œå®ƒç”¨æ¥å­˜å‚¨ä¸€ä¸ªå†…å­˜åŒºåŸŸï¼Œå¯ä»¥åŠ¨æ€è°ƒæ•´è¿™ä¸ªåŒºåŸŸçš„å¤§å°ã€‚

è¯¥ç»“æ„ä½“åŒ…å«ä¸¤ä¸ªæˆå‘˜å˜é‡ï¼š

1. `addr`ï¼šä¸€ä¸ª `uint8_t` ç±»å‹çš„æŒ‡é’ˆï¼ŒæŒ‡å‘ä¸€ä¸ª `uint8_t` ç±»å‹çš„å˜é‡ï¼Œåˆå§‹åŒ–ä¸º `NULL`ã€‚
2. `size`ï¼šä¸€ä¸ª `size_t` ç±»å‹çš„æ•´æ•°ï¼Œåˆå§‹åŒ–ä¸º 0ã€‚

è¯¥ç»“æ„ä½“è¿˜æœ‰ä¸€ä¸ªåä¸º `resize` çš„å‡½æ•°ï¼Œå®ƒçš„å‚æ•°æ˜¯ä¸€ä¸ªè¡¨ç¤ºè¦è°ƒæ•´çš„å¤§å°çš„ `size_t` ç±»å‹çš„æ•´æ•° `len`ï¼Œå‡½æ•°å†…éƒ¨å…ˆæ£€æŸ¥è¯¥å†…å­˜åŒºåŸŸæ˜¯å¦å·²ç»åˆ†é…å¥½ï¼Œå¦‚æœæ²¡æœ‰ï¼Œå°±é‡Šæ”¾å·²ç»åˆ†é…å¥½çš„å†…å­˜ï¼Œç„¶åé€šè¿‡ `posix_memalign` å‡½æ•°æ¥åˆ†é…ä¸€å—è¶³å¤Ÿå¤§çš„å†…å­˜åŒºåŸŸï¼Œå¦‚æœåˆ†é…å¤±è´¥ï¼Œå°±å°† `addr` å˜é‡è®¾ç½®ä¸º `NULL`ï¼Œå¹¶è¿”å›ã€‚æœ€åï¼Œè¯¥å‡½æ•°å°† `addr` å˜é‡æŒ‡å‘æ–°åˆ†é…çš„å†…å­˜åŒºåŸŸï¼Œä»¥ä¾¿ä½¿ç”¨æ–°åˆ†é…çš„å†…å­˜åŒºåŸŸã€‚

è¯¥ç»“æ„ä½“çš„é»˜è®¤æ„é€ å‡½æ•°æ˜¯å®šä¹‰åœ¨ `__attribute__((constructor))` ä¿®é¥°ä¸‹çš„ï¼Œå®ƒä¼šæ‰§è¡Œé»˜è®¤æ„é€ å‡½æ•°ï¼Œå³ `llama_buffer() = default;`ï¼Œè¿™ä¸ªæ„é€ å‡½æ•°åªæ˜¯å°† `addr` å˜é‡åˆå§‹åŒ–ä¸º `NULL`ï¼Œå¹¶ä¸åˆ†é…å†…å­˜ã€‚

è¯¥ç»“æ„ä½“å¯ä»¥ç”¨æ¥å®šä¹‰ä¸€ä¸ªå†…å­˜åŒºåŸŸï¼Œå¹¶é€šè¿‡ `resize` å‡½æ•°æ¥åŠ¨æ€è°ƒæ•´è¿™ä¸ªåŒºåŸŸçš„å¤§å°ã€‚æ³¨æ„ï¼Œåœ¨ä½¿ç”¨è¯¥ç»“æ„ä½“ä¹‹å‰ï¼Œéœ€è¦å…ˆå®šä¹‰å¥½è¯¥å†…å­˜åŒºåŸŸï¼Œå¦åˆ™å°±ä¼šå‡ºç°ç¼–è¯‘é”™è¯¯æˆ–è€…è¿è¡Œæ—¶é”™è¯¯ã€‚


```cpp
struct llama_buffer {
    uint8_t * addr = NULL;
    size_t size = 0;

    llama_buffer() = default;

    void resize(size_t len) {
#ifdef GGML_USE_METAL
        free(addr);
        int result = posix_memalign((void **) &addr, getpagesize(), len);
        if (result == 0) {
            memset(addr, 0, len);
        }
        else {
            addr = NULL;
        }
```

è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º`llama_buffer`çš„ç±»ï¼Œå…¶ä½œç”¨æ˜¯æä¾›äº†ä¸€ä¸ª`llama_buffer`ç±»å‹çš„ç»‘å®šï¼Œå¯ä»¥ç”¨æ¥è¾“å‡ºä¸€ä¸ª`llama_buffer`ç±»å‹çš„å˜é‡ï¼Œè€Œä¸éœ€è¦è¾“å‡ºæºä»£ç ã€‚

å…·ä½“æ¥è¯´ï¼Œè¿™æ®µä»£ç çš„åŠŸèƒ½å¦‚ä¸‹ï¼š

1. å¦‚æœ`GGML_USE_METAL`ä¸ºçœŸï¼Œåˆ™å®šä¹‰äº†ä¸€ä¸ªä»¥`addr`ä¸ºå‚æ•°çš„å‡½æ•°ï¼Œç”¨æ¥åœ¨å†…å­˜ä¸­åˆ é™¤ä¸€ä¸ª`uint8_t`ç±»å‹çš„æŒ‡é’ˆï¼Œå¹¶å°†å…¶å¤åˆ¶åˆ°ä¸€ä¸ªæ–°çš„`uint8_t`ç±»å‹çš„æ•°ç»„ä¸­ã€‚

2. å¦‚æœ`GGML_USE_METAL`ä¸ºå‡ï¼Œåˆ™å®šä¹‰äº†ä¸€ä¸ªä»¥`addr`ä¸ºå‚æ•°çš„å‡½æ•°ï¼Œç”¨æ¥åœ¨å†…å­˜ä¸­åˆ é™¤ä¸€ä¸ª`uint8_t`ç±»å‹çš„æŒ‡é’ˆï¼Œå¹¶å°†å…¶å¤åˆ¶åˆ°ä¸€ä¸ªæ–°çš„`uint8_t`ç±»å‹çš„æ•°ç»„ä¸­ã€‚

3. å®šä¹‰äº†ä¸€ä¸ª`~llama_buffer`å‡½æ•°ï¼Œç”¨æ¥åœ¨`llama_buffer`å¯¹è±¡è¢«åˆ é™¤æ—¶æ‰§è¡Œä¸€äº›æ“ä½œã€‚

4. å®šä¹‰äº†ä¸‰ä¸ªé€šç”¨çš„æˆå‘˜å‡½æ•°é‡è½½å‡½æ•°ï¼Œåˆ†åˆ«ç”¨æ¥ç»‘å®š`llama_buffer`ç±»å‹å˜é‡ã€`llama_buffer`ç±»å‹å˜é‡å’Œ`llama_buffer`ç±»å‹å˜é‡çš„å‰¯æœ¬ï¼Œé¿å…ä¸å¿…è¦çš„æ‹·è´å’Œç§»åŠ¨æ“ä½œã€‚


```cpp
#else
        delete[] addr;
        addr = new uint8_t[len];
#endif
        size = len;
    }

    ~llama_buffer() {
#ifdef GGML_USE_METAL
        free(addr);
#else
        delete[] addr;
#endif
        addr = NULL;
    }

    // disable copy and move
    llama_buffer(const llama_buffer&) = delete;
    llama_buffer(llama_buffer&&) = delete;
    llama_buffer& operator=(const llama_buffer&) = delete;
    llama_buffer& operator=(llama_buffer&&) = delete;
};


```

è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º `kv_cache` çš„ç»“æ„ä½“ï¼Œç”¨äºå­˜å‚¨é”®å€¼å¯¹ `(key, value)`ï¼ŒåŒæ—¶è¯¥ç»“æ„ä½“è¿˜ç»´æŠ¤äº†ä¸€ä¸ªæŒ‡å‘å†…å­˜çš„æŒ‡é’ˆ `ctx` å’Œä¸€ä¸ªæ•´æ•° `n`ã€‚

æ¥ä¸‹æ¥å®šä¹‰äº†ä¸€ä¸ªåä¸º `starcoder_model` çš„ç»“æ„ä½“ï¼Œè¯¥ç»“æ„ä½“åŒ…å«äº†æ¨¡å‹çš„å‚æ•°å’Œæˆå‘˜å˜é‡ã€‚

ç„¶åé€šè¿‡ `ggml_tensor` å’Œ `ggml_context` ç±»å‹çš„æˆå‘˜å˜é‡ï¼Œå®ç°äº†ä»è¾“å…¥åˆ°è¾“å‡ºçš„ä¸€ç»„æ“ä½œã€‚

æ¥ç€é€šè¿‡ `llama_buffer` ç±»å‹çš„æˆå‘˜å˜é‡å®ç°äº†ä»å†…å­˜ä¸­è¯»å–æ•°æ®åˆ°è¾“å…¥æµä¸­çš„æ“ä½œã€‚

æœ€åé€šè¿‡ `struct starcoder_layer` çš„å®¹å™¨ç±»å‹å­˜å‚¨äº†æ¨¡å‹çš„å„ä¸ªéƒ¨åˆ†ï¼Œå¹¶å®ç°äº†å°†éƒ¨åˆ†ä¿¡æ¯é€šè¿‡ `std::vector<starcoder_layer>` çš„å½¢å¼è¿›è¡Œå­˜å‚¨å’Œæ£€ç´¢ã€‚

è¯¥ä»£ç çš„ä½œç”¨æ˜¯ï¼šå®ç°äº†ä¸€ä¸ªç®€å•çš„ StarCoder Modelï¼Œæ”¯æŒä»å†…å­˜ä¸­è¯»å–æ•°æ®åˆ°è¾“å…¥æµä¸­ï¼Œå¹¶å¯¹æ¨¡å‹å‚æ•°å’Œæˆå‘˜å˜é‡è¿›è¡Œç»´æŠ¤ã€‚


```cpp
struct kv_cache {
    struct ggml_tensor * k;
    struct ggml_tensor * v;

    struct ggml_context * ctx = NULL;

    //std::vector<uint8_t> buf;
    llama_buffer buf;

    int n;
};

struct starcoder_model {
    starcoder_hparams hparams;

    // normalization
    struct ggml_tensor * ln_f_g;
    struct ggml_tensor * ln_f_b;

    struct ggml_tensor * wte;     // position embedding
    struct ggml_tensor * wpe;     //    token embedding
    struct ggml_tensor * lm_head; // language model head

    std::vector<starcoder_layer> layers;

    // key + value memory
    //struct ggml_tensor * memory_k;
    //struct ggml_tensor * memory_v;
    struct kv_cache cache;

    // model memory mapped file
    void * mm_addr = NULL;
    uint64_t mm_length = 0;

    //
    struct ggml_context * ctx;
    std::map<std::string, struct ggml_tensor *> tensors;
};

```

è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯å®ç°äº†ä¸€ä¸ªæ–‡ä»¶æ˜ å°„ï¼Œå®ƒå°†ä¸€ä¸ªæŒ‡å®šçš„æ–‡ä»¶åæ˜ å°„åˆ°å†…å­˜ä¸­çš„æŸä¸ªåœ°å€ï¼Œå¹¶è¿”å›è¯¥åœ°å€ã€‚å…·ä½“å®ç°å¦‚ä¸‹ï¼š

1. é¦–å…ˆå®šä¹‰äº†ä¸€ä¸ªåä¸º `mmap_file` çš„å‡½æ•°ï¼Œå®ƒçš„å‚æ•°åŒ…æ‹¬ä¸€ä¸ªæ–‡ä»¶å `fname` å’Œä¸€ä¸ªæŒ‡å‘å†…å­˜ä¸­æ˜ å°„æ–‡ä»¶é•¿åº¦ `mm_length` çš„æŒ‡é’ˆã€‚

2. åœ¨å‡½æ•°å®šä¹‰ä¹‹å‰ï¼Œé€šè¿‡ `#if` æ¡ä»¶æ£€æŸ¥æ˜¯å¦å®šä¹‰äº† `_WIN32` å¹³å°ä»¥åŠ `_POSIX_MAPPED_FILES` æ˜¯å¦ä¸º `0`ã€‚å¦‚æœæ˜¯ `_WIN32`ï¼Œåˆ™ä¸æ‰§è¡Œåç»­ä»£ç ï¼Œå¦‚æœæ˜¯ `_POSIX_MAPPED_FILES`ï¼Œåˆ™å¯ä»¥æ‰§è¡Œåç»­ä»£ç ã€‚

3. å¦‚æœå·²ç»å®šä¹‰äº† `_WIN32` å¹¶ä¸” `_POSIX_MAPPED_FILES` ä¸æ˜¯ `0`ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–‡ä»¶ `fname`ã€‚

4. ä½¿ç”¨ `CreateFileA` å‡½æ•°å°è¯•åˆ›å»ºä¸€ä¸ªæ–‡ä»¶ï¼Œå¹¶æŒ‡å®šæ–‡ä»¶ç±»å‹ä¸º `GENERIC_READ`ã€`FILE_SHARE_READ` å’Œ `FILE_SHARE_WRITE`ã€`FILE_SHARE_DELETE`ï¼ŒåŒæ—¶æŒ‡å®šæ–‡ä»¶ share æ ‡å¿—ä¸º `FILE_SHARE_READ` å’Œ `FILE_SHARE_WRITE`ï¼Œä¸”ä¸åŒ…å«æ–‡ä»¶å†…å®¹ç´¢å¼•ã€‚

5. å¦‚æœåˆ›å»ºæˆåŠŸï¼Œé€šè¿‡ `GetFileSizeEx` å‡½æ•°è·å–æ–‡ä»¶é•¿åº¦ `fileSize`ã€‚

6. åˆ›å»ºä¸€ä¸ªåä¸º `hMapping` çš„æ˜ å°„æ–‡ä»¶ï¼Œå¹¶æŒ‡å®šæ–‡ä»¶ç±»å‹ä¸º `PAGE_READONLY`ï¼ŒåŒæ—¶æŒ‡å®šé¡µå¤§å°ä¸º `0`ã€‚

7. å…³é—­æ–‡ä»¶å’Œæ˜ å°„æ–‡ä»¶ã€‚

8. é€šè¿‡ `MapViewOfFile` å‡½æ•°å°†æ–‡ä»¶æ˜ å°„åˆ°å†…å­˜ä¸­çš„æŸä¸ªåœ°å€ï¼Œå¹¶è¿”å›è¯¥åœ°å€ã€‚

9. æœ€åï¼Œå¦‚æœæ˜ å°„å¤±è´¥ï¼Œè¿”å› `0`ã€‚


```cpp
// From PR #613 (https://github.com/ggerganov/llama.cpp/pull/613)
static void *mmap_file(const char *fname, uint64_t *mm_length) {
#if defined(_WIN32) && !defined(_POSIX_MAPPED_FILES)
    HANDLE hFile = CreateFileA(fname,
                               GENERIC_READ,
                               FILE_SHARE_READ | FILE_SHARE_WRITE | FILE_SHARE_DELETE,
                               NULL,
                               OPEN_EXISTING,
                               FILE_ATTRIBUTE_NORMAL | FILE_ATTRIBUTE_NOT_CONTENT_INDEXED,
                               NULL);
    if (hFile == INVALID_HANDLE_VALUE) return 0;
    LARGE_INTEGER fileSize;
    fileSize.QuadPart = -1;
    GetFileSizeEx(hFile, &fileSize);
    int64_t length = fileSize.QuadPart;
    HANDLE hMapping = CreateFileMappingA(hFile, NULL, PAGE_READONLY, 0, 0, NULL);
    CloseHandle(hFile);
    if (!hMapping) return 0;
    void *addr = MapViewOfFile(hMapping, FILE_MAP_READ, 0, 0, 0);
    CloseHandle(hMapping);
    if (!addr) return 0;
```

è¿™æ®µä»£ç æ˜¯ä¸€ä¸ªCè¯­è¨€å‡½æ•°ï¼Œåä¸º`mmunmap_file`ã€‚å®ƒå®ç°äº†ä¸€ä¸ªå†…å­˜æ˜ å°„å‡½æ•°ï¼Œç”¨äºå°†ä¸€ä¸ªæ–‡ä»¶ä¸­çš„å†…å®¹æ˜ å°„åˆ°ç³»ç»Ÿçš„å†…å­˜ä¸­ã€‚

å‡½æ•°æ¥æ”¶ä¸¤ä¸ªå‚æ•°ï¼šä¸€ä¸ªvoidç±»å‹çš„æŒ‡é’ˆ`addr`ï¼Œè¡¨ç¤ºéœ€è¦æ˜ å°„åˆ°å†…å­˜ä¸­çš„èµ·å§‹åœ°å€ï¼Œä»¥åŠä¸€ä¸ªint64ç±»å‹çš„æ•´æ•°`length`ï¼Œè¡¨ç¤ºæ–‡ä»¶çš„å†…å®¹é•¿åº¦ã€‚å‡½æ•°å†…éƒ¨ä½¿ç”¨è¿™ä¸¤ä¸ªå‚æ•°ï¼Œæ ¹æ®éœ€è¦åœ¨æ–‡ä»¶ä¸­æ‰¾åˆ°èµ·å§‹åœ°å€å’Œå†…å®¹é•¿åº¦ï¼Œå¹¶è¿”å›ä¸€ä¸ªvoidç±»å‹çš„æŒ‡é’ˆ`addr`ï¼Œè¡¨ç¤ºåœ¨å†…å­˜ä¸­çš„èµ·å§‹åœ°å€ã€‚

å‡½æ•°é¦–å…ˆé€šè¿‡è°ƒç”¨Cæ ‡å‡†åº“ä¸­çš„`open`å‡½æ•°ï¼Œä»¥åªè¯»æ–¹å¼æ‰“å¼€ä¸€ä¸ªæ–‡ä»¶`fname`ï¼Œå¹¶è¿”å›å…¶æ–‡ä»¶æè¿°ç¬¦`fd`ã€‚ç„¶åä½¿ç”¨`lseek`å‡½æ•°åœ¨æ–‡ä»¶ä¸­æŸ¥æ‰¾èµ·å§‹åœ°å€ï¼Œå¹¶è¿”å›å®ƒã€‚æ¥ä¸‹æ¥ï¼Œä½¿ç”¨`mmap`å‡½æ•°å°†æ–‡ä»¶å†…å®¹ä»æ–‡ä»¶ä¸­æ˜ å°„åˆ°ç³»ç»Ÿçš„å†…å­˜ä¸­ï¼Œå¹¶è¿”å›ä¸€ä¸ªvoidç±»å‹çš„æŒ‡é’ˆ`addr`ã€‚æœ€åï¼Œä½¿ç”¨`close`å‡½æ•°å…³é—­æ–‡ä»¶ï¼Œå¹¶åœ¨å†…å­˜æ˜ å°„è¿‡ç¨‹ä¸­å‡ºé”™æ—¶è¿”å›0ã€‚

å‡½æ•°è¿˜æœ‰ä¸€ä¸ªè¾…åŠ©å‡½æ•°`munmap_file`ï¼Œå®ƒæ¥æ”¶ä¸¤ä¸ªå‚æ•°ï¼š`addr`å’Œ`length`ã€‚å‡½æ•°é¦–å…ˆä½¿ç”¨`UnmapViewOfFile`å‡½æ•°ä»æ–‡ä»¶ä¸­å¸è½½`addr`æ‰€æŒ‡çš„å†…å­˜ç©ºé—´ï¼Œç„¶åä½¿ç”¨`size_t`ç±»å‹çš„å˜é‡`length`ï¼Œè¡¨ç¤ºæ–‡ä»¶çš„å†…å®¹é•¿åº¦ã€‚è¿™ä¸¤ä¸ªå‡½æ•°æ˜¯åœ¨`munmap_file`å‡½æ•°ä¸­ä½œç”¨ï¼Œç”¨äºç¡®ä¿å‡½æ•°ä½¿ç”¨æ­£ç¡®çš„å†…å­˜åŒºåŸŸï¼Œå¹¶é¿å…è®¿é—®è¶Šç•Œå†…å­˜åŒºåŸŸã€‚


```cpp
#else
    int fd = open(fname, O_RDONLY);
    if (fd == -1) return 0;
    int64_t length = lseek(fd, 0, SEEK_END);
    void *addr = mmap(NULL, length, PROT_READ, MAP_SHARED, fd, 0);
    close(fd);
    if (addr == MAP_FAILED) return 0;
#endif
    *mm_length = length;
    return addr;
}

static void munmap_file(void * addr, size_t length) {
#if defined(_WIN32) && !defined(_POSIX_MAPPED_FILES)
    UnmapViewOfFile(addr);
```

åŸºäºå¯¹é—®é¢˜çš„åˆ†æï¼Œæˆ‘å¾—å‡ºçš„ç­”æ¡ˆæ˜¯Bã€‚


```cpp
#else
    munmap(addr, length);
#endif
}

// load the model's weights from a file
bool starcoder_model_load(const std::string & fname, starcoder_model & model, gpt_vocab & vocab, int32_t n_gpu_layers) {
    printf("%s: loading model from '%s'\n", __func__, fname.c_str());

    auto fin = std::ifstream(fname, std::ios::binary);
    if (!fin) {
        fprintf(stderr, "%s: failed to open '%s'\n", __func__, fname.c_str());
        return false;
    }

    std::vector<char> f_buf(1024*1024);
    fin.rdbuf()->pubsetbuf(f_buf.data(), f_buf.size());


    // verify magic
    {
        uint32_t magic;
        fin.read((char *) &magic, sizeof(magic));
        //if (magic != 0x67676a74) {
        if (magic != 0x67676d6c) {
            fprintf(stderr, "%s: invalid model file '%s' (bad magic)\n", __func__, fname.c_str());
            return false;
        }
    }

    // load hparams
    {
        auto & hparams = model.hparams;

        fin.read((char *) &hparams.n_vocab, sizeof(hparams.n_vocab));
        fin.read((char *) &hparams.n_ctx,   sizeof(hparams.n_ctx));
        fin.read((char *) &hparams.n_embd,  sizeof(hparams.n_embd));
        fin.read((char *) &hparams.n_head,  sizeof(hparams.n_head));
        fin.read((char *) &hparams.n_layer, sizeof(hparams.n_layer));
        fin.read((char *) &hparams.ftype,   sizeof(hparams.ftype));

        const int32_t qntvr = hparams.ftype / GGML_QNT_VERSION_FACTOR;

        printf("%s: n_vocab = %d\n", __func__, hparams.n_vocab);
        printf("%s: n_ctx   = %d\n", __func__, hparams.n_ctx);
        printf("%s: n_embd  = %d\n", __func__, hparams.n_embd);
        printf("%s: n_head  = %d\n", __func__, hparams.n_head);
        printf("%s: n_layer = %d\n", __func__, hparams.n_layer);
        printf("%s: ftype   = %d\n", __func__, hparams.ftype);
        printf("%s: qntvr   = %d\n", __func__, qntvr);

        hparams.ftype %= GGML_QNT_VERSION_FACTOR;
    }

    // load vocab
    {
        int32_t n_vocab = 0;
        fin.read((char *) &n_vocab, sizeof(n_vocab));

        if (n_vocab != model.hparams.n_vocab) {
            fprintf(stderr, "%s: invalid model file '%s' (bad vocab size %d != %d)\n",
                    __func__, fname.c_str(), n_vocab, model.hparams.n_vocab);
            return false;
        }

        std::string word;
        std::vector<char> buf(128);

        for (int i = 0; i < n_vocab; i++) {
            uint32_t len;
            fin.read((char *) &len, sizeof(len));

            buf.resize(len);
            fin.read((char *) buf.data(), len);
            word.assign(buf.data(), len);

            vocab.token_to_id[word] = i;
            vocab.id_to_token[i] = word;

            // if (i < 10) fprintf(stderr, "%.s: vocab[%d] = '%s'\n", __func__, i, word.c_str());
        }

        // Add StarChat special tokens.
        for (std::string token : {
                "<|system|>",
                "<|user|>",
                "<|assistant|>",
                "<|end|>",
                }) {
            if (vocab.token_to_id.find(token) != vocab.token_to_id.end()) {
                vocab.add_special_token(token);
            }
        }
    }

    char *mm_addr = NULL;
    model.mm_addr = mmap_file(fname.c_str(), &model.mm_length);
    if (model.mm_addr == NULL) {
        fprintf(stderr, "%s: failed to mmap '%s'\n", __func__, fname.c_str());
        return false;
    }
    mm_addr = (char *)model.mm_addr;
    fprintf(stderr, "%s: ggml map size = %6.2f MB\n", __func__, model.mm_length/(1024.0*1024.0));

    // for the big tensors, we have the option to store the data in 16-bit floats or quantized
    // in order to save memory and also to speed up the computation
    ggml_type wtype = ggml_ftype_to_ggml_type((ggml_ftype) (model.hparams.ftype));
    if (wtype == GGML_TYPE_COUNT) {
        fprintf(stderr, "%s: invalid model file '%s' (bad ftype value %d)\n",
                __func__, fname.c_str(), model.hparams.ftype);
        return false;
    }

    auto & ctx = model.ctx;

    size_t ctx_size = 0;

    {
        const auto & hparams = model.hparams;

        const int n_layer = hparams.n_layer;


        /*
        const int n_embd  = hparams.n_embd;
        const int n_layer = hparams.n_layer;
        const int n_ctx   = hparams.n_ctx;
        const int n_vocab = hparams.n_vocab;

        const int head_dim = n_embd / hparams.n_head;
        const int kv_heads = hparams.n_head; // 1 if MQA else hparams.n_head
        const int kv_dim   = kv_heads * head_dim;


        ctx_size += n_embd*ggml_type_sizef(GGML_TYPE_F32); // ln_f_g
        ctx_size += n_embd*ggml_type_sizef(GGML_TYPE_F32); // ln_f_b

        ctx_size += n_vocab*n_embd*ggml_type_sizef(wtype);         // wte
        ctx_size +=   n_ctx*n_embd*ggml_type_sizef(GGML_TYPE_F32); // wpe
        ctx_size += n_vocab*n_embd*ggml_type_sizef(wtype);         // lm_head

        ctx_size += n_layer*(n_embd*ggml_type_sizef(GGML_TYPE_F32)); // ln_1_g
        ctx_size += n_layer*(n_embd*ggml_type_sizef(GGML_TYPE_F32)); // ln_1_b

        ctx_size += n_layer*(n_embd*ggml_type_sizef(GGML_TYPE_F32)); // ln_2_g
        ctx_size += n_layer*(n_embd*ggml_type_sizef(GGML_TYPE_F32)); // ln_2_b

        ctx_size += n_layer*((n_embd + 2*kv_dim)*n_embd*ggml_type_sizef(wtype));         // c_attn_attn_w // TODO:
        ctx_size += n_layer*(       (n_embd + 2*kv_dim)*ggml_type_sizef(GGML_TYPE_F32)); // c_attn_attn_b

        ctx_size += n_layer*(n_embd*n_embd*ggml_type_sizef(wtype));           // c_attn_proj_w
        ctx_size += n_layer*(       n_embd*ggml_type_sizef(GGML_TYPE_F32));   // c_attn_proj_b

        ctx_size += n_layer*(4*n_embd*n_embd*ggml_type_sizef(wtype));         // c_mlp_fc_w
        ctx_size += n_layer*(       4*n_embd*ggml_type_sizef(GGML_TYPE_F32)); // c_mlp_fc_b

        ctx_size += n_layer*(4*n_embd*n_embd*ggml_type_sizef(wtype));         // c_mlp_proj_w
        ctx_size += n_layer*(         n_embd*ggml_type_sizef(GGML_TYPE_F32)); // c_mlp_proj_b

        ctx_size += n_ctx*n_layer*n_embd*ggml_type_sizef(GGML_TYPE_F32); // memory_k
        ctx_size += n_ctx*n_layer*n_embd*ggml_type_sizef(GGML_TYPE_F32); // memory_v
        */

        ctx_size += (6 + 12*n_layer)*512; // object overhead

        printf("%s: ggml ctx size = %6.2f MB\n", __func__, ctx_size/(1024.0*1024.0));
    }

    // create the ggml context
    {
        struct ggml_init_params params = {
            /*.mem_size   =*/ ctx_size,
            /*.mem_buffer =*/ NULL,
            /*.no_alloc   =*/ true,
        };

        model.ctx = ggml_init(params);
        if (!model.ctx) {
            fprintf(stderr, "%s: ggml_init() failed\n", __func__);
            return false;
        }
    }

    // prepare memory for the weights
    {
        const auto & hparams = model.hparams;

        const int n_embd  = hparams.n_embd;
        const int n_layer = hparams.n_layer;
        const int n_ctx   = hparams.n_ctx;
        const int n_vocab = hparams.n_vocab;

        const int head_dim = n_embd / hparams.n_head;
        const int kv_heads = hparams.n_head; // 1 if MQA else hparams.n_head
        const int kv_dim   = kv_heads * head_dim;

        model.layers.resize(n_layer);

        model.ln_f_g = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, n_embd);
        model.ln_f_b = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, n_embd);

        model.wte     = ggml_new_tensor_2d(ctx, wtype,         n_embd, n_vocab);
        model.wpe     = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, n_embd, n_ctx);
        model.lm_head = ggml_new_tensor_2d(ctx, wtype,         n_embd, n_vocab);

        // map by name
        model.tensors["model/ln_f/g"] = model.ln_f_g;
        model.tensors["model/ln_f/b"] = model.ln_f_b;

        model.tensors["model/wte"]     = model.wte;
        model.tensors["model/wpe"]     = model.wpe;
        model.tensors["model/lm_head"] = model.lm_head;

        for (int i = 0; i < n_layer; ++i) {
            auto & layer = model.layers[i];

            layer.ln_1_g        = ggml_new_tensor_1d(ctx, GGML_TYPE_F32,   n_embd);
            layer.ln_1_b        = ggml_new_tensor_1d(ctx, GGML_TYPE_F32,   n_embd);

            layer.ln_2_g        = ggml_new_tensor_1d(ctx, GGML_TYPE_F32,   n_embd);
            layer.ln_2_b        = ggml_new_tensor_1d(ctx, GGML_TYPE_F32,   n_embd);

            layer.c_attn_attn_w = ggml_new_tensor_2d(ctx, wtype,           n_embd, n_embd + 2*kv_dim);
            layer.c_attn_attn_b = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, n_embd + 2*kv_dim);

            layer.c_attn_proj_w = ggml_new_tensor_2d(ctx, wtype,           n_embd, n_embd);
            layer.c_attn_proj_b = ggml_new_tensor_1d(ctx, GGML_TYPE_F32,   n_embd);

            layer.c_mlp_fc_w    = ggml_new_tensor_2d(ctx, wtype,           n_embd, 4*n_embd); //TODO: 4*n_embd = config.n_inner
            layer.c_mlp_fc_b    = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 4*n_embd);

            layer.c_mlp_proj_w  = ggml_new_tensor_2d(ctx, wtype,         4*n_embd, n_embd);
            layer.c_mlp_proj_b  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32,   n_embd);

            // map by name
            model.tensors["model/h" + std::to_string(i) + "/ln_1/g"]        = layer.ln_1_g;
            model.tensors["model/h" + std::to_string(i) + "/ln_1/b"]        = layer.ln_1_b;

            model.tensors["model/h" + std::to_string(i) + "/ln_2/g"]        = layer.ln_2_g;
            model.tensors["model/h" + std::to_string(i) + "/ln_2/b"]        = layer.ln_2_b;

            model.tensors["model/h" + std::to_string(i) + "/attn/c_attn/w"] = layer.c_attn_attn_w;
            model.tensors["model/h" + std::to_string(i) + "/attn/c_attn/b"] = layer.c_attn_attn_b;

            model.tensors["model/h" + std::to_string(i) + "/attn/c_proj/w"] = layer.c_attn_proj_w;
            model.tensors["model/h" + std::to_string(i) + "/attn/c_proj/b"] = layer.c_attn_proj_b;

            model.tensors["model/h" + std::to_string(i) + "/mlp/c_fc/w"]    = layer.c_mlp_fc_w;
            model.tensors["model/h" + std::to_string(i) + "/mlp/c_fc/b"]    = layer.c_mlp_fc_b;

            model.tensors["model/h" + std::to_string(i) + "/mlp/c_proj/w"]  = layer.c_mlp_proj_w;
            model.tensors["model/h" + std::to_string(i) + "/mlp/c_proj/b"]  = layer.c_mlp_proj_b;
        }
    }

    // key + value memory
    {
        const auto & hparams = model.hparams;

        const int n_embd  = hparams.n_embd;
        const int n_layer = hparams.n_layer;
        const int n_ctx   = hparams.n_ctx;

        const int n_mem      = n_layer*n_ctx;
        const int n_elements = n_embd*n_mem;

        model.cache.buf.resize(2u*n_elements*ggml_type_size(GGML_TYPE_F16) + 2u*1024*1024);

        struct ggml_init_params c_params;
        c_params.mem_size   = model.cache.buf.size;
        c_params.mem_buffer = model.cache.buf.addr;
        c_params.no_alloc   = false;

        model.cache.ctx = ggml_init(c_params);

        if (!model.cache.ctx) {
            fprintf(stderr, "%s: failed to allocate memory for kv cache\n", __func__);
            return false;
        }

        model.cache.k = ggml_new_tensor_1d(model.cache.ctx, GGML_TYPE_F16, n_elements);
        model.cache.v = ggml_new_tensor_1d(model.cache.ctx, GGML_TYPE_F16, n_elements);

        const size_t memory_size = ggml_nbytes(model.cache.k) + ggml_nbytes(model.cache.v);

        printf("%s: kv_cache memory size = %8.2f MB, n_mem = %d\n", __func__, memory_size/1024.0/1024.0, n_mem);
    }

    // load weights
    {
        size_t total_size = 0;

        bool has_lm_head = false;

        while (true) {
            int32_t n_dims;
            int32_t length;
            int32_t ttype;

            fin.read(reinterpret_cast<char *>(&n_dims), sizeof(n_dims));
            fin.read(reinterpret_cast<char *>(&length), sizeof(length));
            fin.read(reinterpret_cast<char *>(&ttype),  sizeof(ttype));

            if (fin.eof()) {
                break;
            }

            int32_t nelements = 1;
            int32_t ne[2] = { 1, 1 };
            for (int i = 0; i < n_dims; ++i) {
                fin.read(reinterpret_cast<char *>(&ne[i]), sizeof(ne[i]));
                nelements *= ne[i];
            }

            std::string name(length, 0);
            fin.read(&name[0], length);

            if (model.tensors.find(name.data()) == model.tensors.end()) {
                fprintf(stderr, "%s: unknown tensor '%s' in model file\n", __func__, name.data());
                return false;
            }

            auto tensor = model.tensors[name.data()];

            if (tensor->ne[0] != ne[0] || tensor->ne[1] != ne[1]) {
                fprintf(stderr, "%s: tensor '%s' has wrong shape in model file: got [%d, %d], expected [%d, %d]\n",
                        __func__, name.data(), (int) tensor->ne[0], (int) tensor->ne[1], ne[0], ne[1]);
                return false;
            }
            if (ggml_nelements(tensor) != nelements) {
                fprintf(stderr, "%s: tensor '%s' has wrong size in model file. got %d, expected %d\n",
                        __func__, name.data(), (int) ggml_nelements(tensor), nelements);
                return false;
            }

            // for debugging
            if (0) {
                printf("%24s - [%5d, %5d], type = %6s, %6.2f MB, %9zu bytes\n", name.data(), ne[0], ne[1], ggml_type_name(ggml_type(ttype)), ggml_nbytes(tensor)/1024.0/1024.0, ggml_nbytes(tensor));
            }

            const size_t bpe = ggml_type_size(ggml_type(ttype));

            if ((nelements*bpe)/ggml_blck_size(tensor->type) != ggml_nbytes(tensor)) {
                fprintf(stderr, "%s: tensor '%s' has wrong size in model file: got %zu, expected %zu\n",
                        __func__, name.data(), ggml_nbytes(tensor), nelements*bpe);
                return false;
            }

            // mmap
            size_t offset = fin.tellg();
            size_t tensor_data_size = ggml_nbytes(tensor);
            //offset = (offset + 31) & -32;
            tensor->data = mm_addr + offset;
            fin.seekg(offset + tensor_data_size);
            total_size += tensor_data_size;

            // GPT-2 models share the WTE tensor as the LM head
            if (name == "model/wte" && has_lm_head == false) {
                // Dont know if this is required, test models have an lm_head
                model.lm_head->data = tensor->data;
            }

            if (name == "model/lm_head") {
                has_lm_head = true;
            }
        }

        printf("%s: model size  = %8.2f MB\n", __func__, total_size/1024.0/1024.0);
    }

    fin.close();

```

This code appears to be a function definition for the CUDA layer layer that offloads the computation of the last layer's activations to a GPU device. It appears to take in a layer configuration struct and a vector of weights in the output of the last layer and outputs the total amount of VRAM used by the function. The function also disables the scratch memory and sets the scratch index to 0. The scratch memory is used to store the activations of the last layer before offloading them to the GPU device.


```cpp
#ifdef GGML_USE_CUBLAS
    {
        const auto & hparams = model.hparams;
        const int n_gpu = std::min(n_gpu_layers, int(hparams.n_layer));

        fprintf(stderr, "%s: [cublas] offloading %d layers to GPU\n", __func__, n_gpu);

        size_t vram_total = 0;

        for (int i = 0; i < n_gpu; ++i) {
            const auto & layer = model.layers[i];

            layer.c_attn_attn_w->backend = GGML_BACKEND_GPU;
            ggml_cuda_transform_tensor((uint8_t *)layer.c_attn_attn_w->data, layer.c_attn_attn_w); vram_total += ggml_nbytes(layer.c_attn_attn_w);

            layer.c_attn_proj_w->backend = GGML_BACKEND_GPU;
            ggml_cuda_transform_tensor((uint8_t *)layer.c_attn_proj_w->data, layer.c_attn_proj_w); vram_total += ggml_nbytes(layer.c_attn_proj_w);

            layer.c_mlp_fc_w->backend = GGML_BACKEND_GPU;
            ggml_cuda_transform_tensor((uint8_t *)layer.c_mlp_fc_w->data, layer.c_mlp_fc_w); vram_total += ggml_nbytes(layer.c_mlp_fc_w);

            layer.c_mlp_proj_w->backend = GGML_BACKEND_GPU;
            ggml_cuda_transform_tensor((uint8_t *)layer.c_mlp_proj_w->data, layer.c_mlp_proj_w); vram_total += ggml_nbytes(layer.c_mlp_proj_w);
        }

        ggml_cuda_set_scratch_size(0); // disable scratch

        //if (n_gpu_layers > (int) hparams.n_layer) {
        //    fprintf(stderr, "%s: [cublas] offloading output layer to GPU\n", __func__);
        //    ggml_cuda_transform_tensor(model.output); vram_total += ggml_nbytes(model.output);
        //}

        fprintf(stderr, "%s: [cublas] total VRAM used: %zu MB\n", __func__, vram_total / 1024 / 1024);
    }
```

This is a open source software, it seems to be using NVLink to offload the computation to the GPU, as well as using CLI to perform the computation.

The function `opencl_nvlink_allocate_model` is used to allocate the model to the NVLink and CLI.

The function `opencl_minimize_vrams` is used to minimize the amount of VRAM usage by offloading the computation to the GPU. This function takes in two arguments, `n_gpu_layers` and `hparams.n_layer`.

It appears that the function first checks the number of GPU layers and uses the smaller number if the number of layers is less than the number of GPU layers.

Then it loops through the layers of the model and sets the backend of the layer to GGML_BACKEND_GPU if it's on the GPU, and sets the Transform of the Attention and Mlp to use the GPU.

It then uses ggml_cl_transform\_tensor to convert the data from the Attention and Mlp to the GPU, and updates the Vram\_total variable with the amount of memory used.

It finally returns true, indicating that the function has been executed correctly.


```cpp
#elif defined(GGML_USE_CLBLAST)
    //From koboldcpp
    {
        const auto & hparams = model.hparams;
        size_t vram_total = 0;
        const int n_gpu = std::min(n_gpu_layers, int(hparams.n_layer));
        fprintf(stderr, "%s: [opencl] offloading %d layers to GPU\n", __func__, n_gpu);
        for (int i = 0; i < n_gpu; ++i) {
            const auto & layer = model.layers[i];
            layer.c_attn_attn_w->backend = GGML_BACKEND_GPU;
            layer.c_attn_proj_w->backend = GGML_BACKEND_GPU;
            layer.c_mlp_fc_w->backend = GGML_BACKEND_GPU;
            layer.c_mlp_proj_w->backend = GGML_BACKEND_GPU;
            ggml_cl_transform_tensor(layer.c_attn_attn_w->data,layer.c_attn_attn_w); vram_total += ggml_nbytes(layer.c_attn_attn_w);
            ggml_cl_transform_tensor(layer.c_attn_proj_w->data,layer.c_attn_proj_w); vram_total += ggml_nbytes(layer.c_attn_proj_w);
            ggml_cl_transform_tensor(layer.c_mlp_fc_w->data,layer.c_mlp_fc_w); vram_total += ggml_nbytes(layer.c_mlp_fc_w);
            ggml_cl_transform_tensor(layer.c_mlp_proj_w->data,layer.c_mlp_proj_w); vram_total += ggml_nbytes(layer.c_mlp_proj_w);
        }
        fprintf(stderr, "%s: [opencl] total VRAM used: %zu MB\n", __func__, vram_total / 1024 / 1024);
    }
    #endif

    return true;
}

```

This function appears to perform a simple language model forward and calculate the logits for a given input sequence.

It first loops through the input sequence and performs a pointwise product with the first token to create a non-linear space for the input.

Then, it performs a softmax operation on this input to convert it to a probability distribution over the vocabulary.

Finally, it runs the computation for the input sequence and returns the result, including the logits for each token.

It is based on the model you provided, which is a simplified version of the language model architecture proposed by Girshick.


```cpp
// evaluate the transformer
//
//   - model:     the model
//   - n_threads: number of threads to use
//   - n_past:    the context size so far
//   - embd_inp:  the embeddings of the tokens in the context
//   - embd_w:    the predicted logits for the next token
//
bool starcoder_eval(
        const starcoder_model & model,
        const int n_threads,
        const int n_past,
        const std::vector<gpt_vocab::id> & embd_inp,
              std::vector<float>         & embd_w,
              size_t                     & mem_per_token) {

    const int N = int(embd_inp.size());

    const auto & hparams = model.hparams;

    auto & cache = model.cache;

    const int n_embd  = hparams.n_embd;
    const int n_layer = hparams.n_layer;
    const int n_ctx   = hparams.n_ctx;
    const int n_head  = hparams.n_head;
    const int n_vocab = hparams.n_vocab;

    // Scratch is too small for large n_batch (256)
    //static size_t buf_size = 256u*1024*1024;
    static size_t buf_size = 256u*1024*1024*2;
    static void * buf = malloc(buf_size);

    // use 2 scratch buffers
    // TODO: very hacky solution - reimplement in a more elegant way
    static size_t scratch0_size = 256u*1024*1024*2;
    static void * scratch0 = malloc(scratch0_size);

    static size_t scratch1_size = 256u*1024*1024*2;
    static void * scratch1 = malloc(scratch1_size);

    if (mem_per_token > 0 && mem_per_token*N > buf_size) {
        const size_t buf_size_new = size_t(1.1*(mem_per_token*N)); // add 10% to account for ggml object overhead
        printf("\n%s: reallocating buffer from %zu to %zu bytes\n", __func__, buf_size, buf_size_new);

        // reallocate
        buf_size = buf_size_new;
        buf = realloc(buf, buf_size);
        if (buf == nullptr) {
            fprintf(stderr, "%s: failed to allocate %zu bytes\n", __func__, buf_size);
            return false;
        }
    }

    struct ggml_init_params params = {
        /*.mem_size   =*/ buf_size,
        /*.mem_buffer =*/ buf,
        /*.no_alloc   =*/ false,
    };

    struct ggml_context * ctx0 = ggml_init(params);
    struct ggml_cgraph * gf = ggml_new_graph(ctx0);

    struct ggml_tensor * embd = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, N);


    memcpy(embd->data, embd_inp.data(), N*ggml_element_size(embd));

    struct ggml_tensor * position = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, N);
    for (int i = 0; i < N; ++i) {
        ((int32_t *) position->data)[i] = n_past + i;
    }

    // wte + wpe
    struct ggml_tensor * inpL =
        ggml_add(ctx0,
                ggml_get_rows(ctx0, model.wte, embd),
                ggml_get_rows(ctx0, model.wpe, position));

    for (int il = 0; il < n_layer; ++il) {
        struct ggml_tensor * cur;

        ggml_set_scratch(ctx0, { 0, scratch0_size, scratch0, });

        // norm
        {
            // [ 768, N]
            cur = ggml_norm(ctx0, inpL, hparams.eps);

            // cur = ln_1_g*cur + ln_1_b
            // [ 768, N]
            cur = ggml_add(ctx0,
                    ggml_mul(ctx0,
                        ggml_repeat(ctx0, model.layers[il].ln_1_g, cur),
                        cur),
                    ggml_repeat(ctx0, model.layers[il].ln_1_b, cur));
        }

        // attn
        // [2304, 768] - model.layers[il].c_attn_attn_w
        // [2304,   1] - model.layers[il].c_attn_attn_b
        // [ 768,   N] - cur (in)
        // [2304,   N] - cur (out)
        //
        // cur = attn_w*cur + attn_b
        // [2304, N]
        {
            cur = ggml_mul_mat(ctx0,
                    model.layers[il].c_attn_attn_w,
                    cur);

            cur = ggml_add(ctx0,
                    ggml_repeat(ctx0, model.layers[il].c_attn_attn_b, cur),
                    cur);
        }

        // self-attention
        {
            struct ggml_tensor * Qcur = ggml_view_2d(ctx0, cur, n_embd, N, cur->nb[1], 0*sizeof(float)*n_embd);
            struct ggml_tensor * Kcur = ggml_view_2d(ctx0, cur, n_embd, N, cur->nb[1], 1*sizeof(float)*n_embd);
            struct ggml_tensor * Vcur = ggml_view_2d(ctx0, cur, n_embd, N, cur->nb[1], 2*sizeof(float)*n_embd);

            // store key and value to memory
            if (N >= 1) {
                struct ggml_tensor * k = ggml_view_1d(ctx0, cache.k, N*n_embd, (ggml_element_size(cache.k)*n_embd)*(il*n_ctx + n_past));
                struct ggml_tensor * v = ggml_view_1d(ctx0, cache.v, N*n_embd, (ggml_element_size(cache.v)*n_embd)*(il*n_ctx + n_past));

                ggml_build_forward_expand(gf, ggml_cpy(ctx0, Kcur, k));
                ggml_build_forward_expand(gf, ggml_cpy(ctx0, Vcur, v));
            }

            // Q = Qcur.contiguous().view(n_embd/n_head, n_head, N).permute(0, 2, 1, 3)
            // [64, N, 12]
            struct ggml_tensor * Q =
                ggml_permute(ctx0,
                        ggml_cpy(ctx0,
                            Qcur,
                            ggml_new_tensor_3d(ctx0, GGML_TYPE_F32, n_embd/n_head, n_head, N)),
                        0, 2, 1, 3);

            // K = Kmem.view(n_embd/n_head, n_head, n_past + N).permute(0, 2, 1, 3)
            // [64, n_past + N, 12]
            struct ggml_tensor * K =
                ggml_permute(ctx0,
                        ggml_reshape_3d(ctx0,
                            ggml_view_1d(ctx0, cache.k, (n_past + N)*n_embd, il*n_ctx*ggml_element_size(cache.k)*n_embd),
                            n_embd/n_head, n_head, n_past + N),
                        0, 2, 1, 3); //TODO: need to be tiled

            // GG: flash attention
            //struct ggml_tensor * V =
            //    ggml_cpy(ctx0,
            //            ggml_permute(ctx0,
            //                ggml_reshape_3d(ctx0,
            //                    ggml_view_1d(ctx0, model.memory_v, (n_past + N)*n_embd, il*n_ctx*ggml_element_size(model.memory_v)*n_embd),
            //                    n_embd/n_head, n_head, n_past + N),
            //                1, 2, 0, 3),
            //            ggml_new_tensor_3d(ctx0, GGML_TYPE_F32, n_past + N, n_embd/n_head, n_head));

            //struct ggml_tensor * KQV = ggml_flash_attn(ctx0, Q, K, V, true);

            // K * Q
            // [n_past + N, N, 12]
            struct ggml_tensor * KQ = ggml_mul_mat(ctx0, K, Q); //TODO: check if it broadcasts

            // KQ_scaled = KQ / sqrt(n_embd/n_head)
            // [n_past + N, N, 12]
            struct ggml_tensor * KQ_scaled =
                ggml_scale_inplace(ctx0,
                        KQ,
                        ggml_new_f32(ctx0, 1.0f/sqrt(float(n_embd)/n_head))
                        );

            // KQ_masked = mask_past(KQ_scaled)
            // [n_past + N, N, 12]
            struct ggml_tensor * KQ_masked = ggml_diag_mask_inf_inplace(ctx0, KQ_scaled, n_past);

            // KQ = soft_max(KQ_masked)
            // [n_past + N, N, 12]
            struct ggml_tensor * KQ_soft_max = ggml_soft_max_inplace(ctx0, KQ_masked);

            // V_trans = Vmem.view(n_embd/n_head, n_head, n_past + N).permute(1, 2, 0, 3).contiguous()
            // [n_past + N, 64, 12]
            struct ggml_tensor * V_trans =
                ggml_cpy(ctx0,
                        ggml_permute(ctx0,
                            ggml_reshape_3d(ctx0,
                                ggml_view_1d(ctx0, cache.v, (n_past + N)*n_embd, il*n_ctx*ggml_element_size(cache.v)*n_embd),
                                n_embd/n_head, n_head, n_past + N),
                            1, 2, 0, 3),
                        ggml_new_tensor_3d(ctx0, cache.v->type, n_past + N, n_embd/n_head, n_head));

            // KQV = transpose(V) * KQ_soft_max
            // [64, N, 12]
            struct ggml_tensor * KQV = ggml_mul_mat(ctx0, V_trans, KQ_soft_max);

            // KQV_merged = KQV.permute(0, 2, 1, 3)
            // [64, 12, N]
            struct ggml_tensor * KQV_merged = ggml_permute(ctx0, KQV, 0, 2, 1, 3);

            // cur = KQV_merged.contiguous().view(n_embd, N)
            // [768, N]
            cur = ggml_cpy(ctx0,
                    KQV_merged,
                    ggml_new_tensor_2d(ctx0, GGML_TYPE_F32, n_embd, N));
        }

        // projection
        // [ 768, 768] - model.layers[il].c_attn_proj_w
        // [ 768,   1] - model.layers[il].c_attn_proj_b
        // [ 768,   N] - cur (in)
        // [ 768,   N] - cur (out)
        //
        // cur = proj_w*cur + proj_b
        // [768, N]
        {
            cur = ggml_mul_mat(ctx0,
                    model.layers[il].c_attn_proj_w,
                    cur);

            cur = ggml_add(ctx0,
                    ggml_repeat(ctx0, model.layers[il].c_attn_proj_b, cur),
                    cur);
        }

        // add the input
        cur = ggml_add(ctx0, cur, inpL);

        struct ggml_tensor * inpFF = cur;

        ggml_set_scratch(ctx0, { 0, scratch1_size, scratch1, });

        // feed-forward network
        {
            // norm
            {
                cur = ggml_norm(ctx0, inpFF, hparams.eps);

                // cur = ln_2_g*cur + ln_2_b
                // [ 768, N]
                cur = ggml_add(ctx0,
                        ggml_mul(ctx0,
                            ggml_repeat(ctx0, model.layers[il].ln_2_g, cur),
                            cur),
                        ggml_repeat(ctx0, model.layers[il].ln_2_b, cur));
            }

            // fully connected
            // [3072, 768] - model.layers[il].c_mlp_fc_w
            // [3072,   1] - model.layers[il].c_mlp_fc_b
            // [ 768,   N] - cur (in)
            // [3072,   N] - cur (out)
            //
            // cur = fc_w*cur + fc_b
            // [3072, N]
            cur = ggml_mul_mat(ctx0,
                    model.layers[il].c_mlp_fc_w,
                    cur);

            cur = ggml_add(ctx0,
                    ggml_repeat(ctx0, model.layers[il].c_mlp_fc_b, cur),
                    cur);

            // GELU activation
            // [3072, N]
            cur = ggml_gelu(ctx0, cur);

            // projection
            // [ 768, 3072] - model.layers[il].c_mlp_proj_w
            // [ 768,    1] - model.layers[il].c_mlp_proj_b
            // [3072,    N] - cur (in)
            // [ 768,    N] - cur (out)
            //
            // cur = proj_w*cur + proj_b
            // [768, N]
            cur = ggml_mul_mat(ctx0,
                    model.layers[il].c_mlp_proj_w,
                    cur);

            cur = ggml_add(ctx0,
                    ggml_repeat(ctx0, model.layers[il].c_mlp_proj_b, cur),
                    cur);
        }

        // input for next layer
        inpL = ggml_add(ctx0, cur, inpFF);
    }

    ggml_set_scratch(ctx0, { 0, scratch0_size, scratch0, });

    // norm
    {
        // [ 768, N]
        inpL = ggml_norm(ctx0, inpL, hparams.eps);

        // inpL = ln_f_g*inpL + ln_f_b
        // [ 768, N]
        inpL = ggml_add(ctx0,
                ggml_mul(ctx0,
                    ggml_repeat(ctx0, model.ln_f_g, inpL),
                    inpL),
                ggml_repeat(ctx0, model.ln_f_b, inpL));
    }

    ggml_set_scratch(ctx0, { 0, 0, nullptr, });

    // inpL = WTE * inpL
    // [ 768, 50257] - model.lm_head
    // [ 768, N]     - inpL
    inpL = ggml_mul_mat(ctx0, model.lm_head, inpL);

    // logits -> probs
    //inpL = ggml_soft_max_inplace(ctx0, inpL);

    // run the computation
    ggml_build_forward_expand(gf, inpL);
    ggml_graph_compute_with_ctx(ctx0, gf, n_threads);

    //if (n_past%100 == 0) {
    //    ggml_graph_print   (&gf);
    //    ggml_graph_dump_dot(&gf, NULL, "gpt-2.dot");
    //}

    //embd_w.resize(n_vocab*N);
    //memcpy(embd_w.data(), ggml_get_data(inpL), sizeof(float)*n_vocab*N);

    // return result just for the last token
    embd_w.resize(n_vocab);
    memcpy(embd_w.data(), (float *) ggml_get_data(inpL) + (n_vocab*(N-1)), sizeof(float)*n_vocab);

    if (mem_per_token == 0) {
        mem_per_token = ggml_used_mem(ctx0)/N;
    }
    //printf("used_mem = %zu MB\n", ggml_used_mem(ctx0)/(1024*1024));

    ggml_free(ctx0);

    return true;
}


```

This is a C++ function that performs model training on a text-based data source. It takes in an embedded document (embd) and a training dataset (dataset), and trains a model for a specific task using the STarcoder algorithm. 

The function starts by iterating through the embedded document and each token. For each token, it checks if it is a model-specific token (i.e. a "<|end|>" token for STarcoder) or a StarChat-specific token (i.e. a "starcoder<|end|>" token). If the token is not a model-specific or StarChat-specific token, it skips to the next iteration. 

If the token is a model-specific token, it performs some additional checks. If the token is a STarcoder "<|end|>" token, it will train the model for the STarcoder algorithm. If the token is not a STarcoder "<|end|>" token but is a StarChat-specific token, it will handle the StarChat "<|end|>" token.

The function also includes some additional code to report the timing of the training process. It reports the mem used for each token, the load time taken to process the entire dataset, and the sample time taken to process each token. It also checks if the input prompt should beSubracted, and it also reports the total time taken to train the model.

The function also includes some internal functions that are not defined in the code provided. For example, it includes a function called ggml\_time\_us() that returns the current US time as a number of milliseconds. It also includes a function called ggml\_free() that frees the memory associated with the embedded document.


```cpp
int main(int argc, char ** argv) {
    ggml_time_init();

    const int64_t t_main_start_us = ggml_time_us();

    gpt_params params;
    params.model = "models/gpt-2-117M/ggml-model.bin";

    if (gpt_params_parse(argc, argv, params) == false) {
        return 1;
    }

    if (params.seed < 0) {
        params.seed = int(time(NULL));
    }

    printf("%s: seed = %d\n", __func__, params.seed);

    std::mt19937 rng(params.seed);
    if (params.prompt.empty()) {
        params.prompt = gpt_random_prompt(rng);
    }

    int64_t t_load_us = 0;

    gpt_vocab vocab;
    starcoder_model model;

    // load the model
    {
        const int64_t t_start_us = ggml_time_us();

        if (!starcoder_model_load(params.model, model, vocab, params.n_gpu_layers)) {
            fprintf(stderr, "%s: failed to load model from '%s'\n", __func__, params.model.c_str());
            return 1;
        }

        t_load_us = ggml_time_us() - t_start_us;

        test_gpt_tokenizer(vocab, params.token_test);
    }

    int n_past = 0;

    int64_t t_sample_us  = 0;
    int64_t t_predict_us = 0;

    std::vector<float> logits;

    // tokenize the prompt
    std::vector<gpt_vocab::id> embd_inp = ::gpt_tokenize(vocab, params.prompt);

    params.n_predict = std::min(params.n_predict, model.hparams.n_ctx - (int) embd_inp.size());

    printf("%s: prompt: '%s'\n", __func__, params.prompt.c_str());
    printf("%s: number of tokens in prompt = %zu\n", __func__, embd_inp.size());
    for (size_t i = 0; i < embd_inp.size(); i++) {
        printf("%s: token[%zu] = %6d, %s\n", __func__, i, embd_inp[i], vocab.id_to_token.at(embd_inp[i]).c_str());
    }
    printf("\n\n");

    // Handle StarChat "<|end|>" token.
    gpt_vocab::id starchat_end_token = -1;
    {
        const auto it = vocab.token_to_id.find("<|end|>");
        if (it != vocab.token_to_id.end()) {
            starchat_end_token = it->second;
        }
    }

    // submit the input prompt token-by-token
    // this reduces the memory usage during inference, at the cost of a bit of speed at the beginning
    std::vector<gpt_vocab::id> embd;

    // determine the required inference memory per token:
    size_t mem_per_token = 0;
    printf("Calling starcoder_eval\n");
    starcoder_eval(model, params.n_threads, 0, { 0, 1, 2, 3 }, logits, mem_per_token);

    for (size_t i = embd.size(); i < embd_inp.size() + params.n_predict; i++) {
        // predict
        if (embd.size() > 0) {
            const int64_t t_start_us = ggml_time_us();

            if (!starcoder_eval(model, params.n_threads, n_past, embd, logits, mem_per_token)) {
                printf("Failed to predict\n");
                return 1;
            }

            // Should input processing count towards t_predict?
            if (i > embd_inp.size()) {
                t_predict_us += ggml_time_us() - t_start_us;
            }
        }

        n_past += int(embd.size());
        embd.clear();

        if (i >= embd_inp.size()) {
            // sample next token
            const int   top_k = params.top_k;
            const float top_p = params.top_p;
            const float temp  = params.temp;

            const int n_vocab = model.hparams.n_vocab;

            gpt_vocab::id id = 0;

            {
                const int64_t t_start_sample_us = ggml_time_us();

                id = gpt_sample_top_k_top_p(vocab, logits.data() + (logits.size() - n_vocab), top_k, top_p, temp, rng);

                t_sample_us += ggml_time_us() - t_start_sample_us;
            }

            // add it to the context
            embd.push_back(id);
        } else {
            // if here, it means we are still processing the input prompt
            for (size_t k = i; k < embd_inp.size(); k++) {
                embd.push_back(embd_inp[k]);
                if (int32_t(embd.size()) >= params.n_batch) {
                    break;
                }
            }
            i += int(embd.size()) - 1;
        }

        // display text
        for (auto id : embd) {
            printf("%s", vocab.id_to_token[id].c_str());
        }
        fflush(stdout);

        // check if model is santacoder
        if (model.hparams.n_layer <= 30 && embd.back() == 49152) {
            break;
        }
        // check if model is starcoder
        else if (embd.back() == 0) { //TODO: this is only for starcoder
            break;
        }
        // Handle StarChat "<|end|>" token.
        else if (embd.back() == starchat_end_token) {
            //break;
        }
    }

    // report timing
    {
        const int64_t t_main_end_us = ggml_time_us();

        printf("\n\n");
        printf("%s: mem per token = %8zu bytes\n", __func__, mem_per_token);
        printf("%s:     load time = %8.2f ms\n", __func__, t_load_us/1000.0f);
        printf("%s:   sample time = %8.2f ms\n", __func__, t_sample_us/1000.0f);
        //Shouldnt the input prompt be subracted?
        printf("%s:  predict time = %8.2f ms / %.2f ms per token\n", __func__, t_predict_us/1000.0f, t_predict_us/1000.0f/(n_past - embd_inp.size()));
        //printf("%s:  predict time = %8.2f ms / %.2f ms per token\n", __func__, t_predict_us/1000.0f, t_predict_us/1000.0f/n_past);

        printf("%s:    total time = %8.2f ms\n", __func__, (t_main_end_us - t_main_start_us)/1000.0f);
    }

    ggml_free(model.ctx);

    if (model.mm_addr) {
	    munmap_file(model.mm_addr, model.mm_length);
    }

    return 0;
}

```